{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import librosa\n",
    "import librosa.display\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from matplotlib.pyplot import specgram\n",
    "import keras\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding\n",
    "from keras.layers import LSTM\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Input, Flatten, Dropout, Activation\n",
    "from keras.layers import Conv1D, MaxPooling1D, AveragePooling1D\n",
    "from keras.models import Model\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import regularizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mylist= os.listdir('data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(mylist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "03-01-01-01-01-01-02.wav\n"
     ]
    }
   ],
   "source": [
    "print(mylist[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "03\n"
     ]
    }
   ],
   "source": [
    "print(mylist[400][6:-16])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting the audio file's waveform and its spectrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, sampling_rate = librosa.load('data/03-01-01-01-01-01-02.wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PolyCollection at 0x1e559b7a9e8>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import glob \n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "librosa.display.waveplot(data, sr=sampling_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\reddyvs\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:7: WavFileWarning: Chunk (non-data) not understood, skipping it.\n",
      "  import sys\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Incomplete wav chunk.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-825578bccbf4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0msr\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscipy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwavfile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'data/03-01-01-01-01-01-02.wav'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;31m## Parameters: 10ms step, 30ms window\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\scipy\\io\\wavfile.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(filename, mmap)\u001b[0m\n\u001b[0;32m    287\u001b[0m                     \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Unexpected end of file.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    288\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchunk_id\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 289\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Incomplete wav chunk.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    290\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    291\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mchunk_id\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34mb'fmt '\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Incomplete wav chunk."
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import scipy.io.wavfile\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "\n",
    "sr,x = scipy.io.wavfile.read('data/03-01-01-01-01-01-02.wav')\n",
    "\n",
    "## Parameters: 10ms step, 30ms window\n",
    "nstep = int(sr * 0.01)\n",
    "nwin  = int(sr * 0.03)\n",
    "nfft = nwin\n",
    "\n",
    "window = np.hamming(nwin)\n",
    "\n",
    "## will take windows x[n1:n2].  generate\n",
    "## and loop over n2 such that all frames\n",
    "## fit within the waveform\n",
    "nn = range(nwin, len(x), nstep)\n",
    "\n",
    "X = np.zeros( (len(nn), nfft//2) )\n",
    "\n",
    "for i,n in enumerate(nn):\n",
    "    xseg = x[n-nwin:n]\n",
    "    z = np.fft.fft(window * xseg, nfft)\n",
    "    X[i,:] = np.log(np.abs(z[:nfft//2]))\n",
    "\n",
    "plt.imshow(X.T, interpolation='nearest',\n",
    "    origin='lower',\n",
    "    aspect='auto')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "feeling_list=[]\n",
    "for item in mylist:\n",
    "    if item[6:-16]=='02' and int(item[18:-4])%2==0:\n",
    "        feeling_list.append('female_calm')\n",
    "    elif item[6:-16]=='02' and int(item[18:-4])%2==1:\n",
    "        feeling_list.append('male_calm')\n",
    "    elif item[6:-16]=='03' and int(item[18:-4])%2==0:\n",
    "        feeling_list.append('female_happy')\n",
    "    elif item[6:-16]=='03' and int(item[18:-4])%2==1:\n",
    "        feeling_list.append('male_happy')\n",
    "    elif item[6:-16]=='04' and int(item[18:-4])%2==0:\n",
    "        feeling_list.append('female_sad')\n",
    "    elif item[6:-16]=='04' and int(item[18:-4])%2==1:\n",
    "        feeling_list.append('male_sad')\n",
    "    elif item[6:-16]=='05' and int(item[18:-4])%2==0:\n",
    "        feeling_list.append('female_angry')\n",
    "    elif item[6:-16]=='05' and int(item[18:-4])%2==1:\n",
    "        feeling_list.append('male_angry')\n",
    "    elif item[6:-16]=='06' and int(item[18:-4])%2==0:\n",
    "        feeling_list.append('female_fearful')\n",
    "    elif item[6:-16]=='06' and int(item[18:-4])%2==1:\n",
    "        feeling_list.append('male_fearful')\n",
    "    elif item[:1]=='a':\n",
    "        feeling_list.append('male_angry')\n",
    "    elif item[:1]=='f':\n",
    "        feeling_list.append('male_fearful')\n",
    "    elif item[:1]=='h':\n",
    "        feeling_list.append('male_happy')\n",
    "    #elif item[:1]=='n':\n",
    "        #feeling_list.append('neutral')\n",
    "    elif item[:2]=='sa':\n",
    "        feeling_list.append('male_sad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = pd.DataFrame(feeling_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>male_calm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>female_calm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>male_calm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>female_calm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>male_calm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>female_calm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>male_calm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>female_calm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>male_calm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>female_calm</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             0\n",
       "0    male_calm\n",
       "1  female_calm\n",
       "2    male_calm\n",
       "3  female_calm\n",
       "4    male_calm\n",
       "5  female_calm\n",
       "6    male_calm\n",
       "7  female_calm\n",
       "8    male_calm\n",
       "9  female_calm"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the features of audio files using librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns=['feature'])\n",
    "bookmark=0\n",
    "for index,y in enumerate(mylist):\n",
    "    if mylist[index][6:-16]!='01' and mylist[index][6:-16]!='07' and mylist[index][6:-16]!='08' and mylist[index][:2]!='su' and mylist[index][:1]!='n' and mylist[index][:1]!='d':\n",
    "        X, sample_rate = librosa.load('data/'+y, res_type='kaiser_fast',duration=2.5,sr=22050*2,offset=0.5)\n",
    "        sample_rate = np.array(sample_rate)\n",
    "        mfccs = np.mean(librosa.feature.mfcc(y=X, \n",
    "                                            sr=sample_rate, \n",
    "                                            n_mfcc=13),\n",
    "                        axis=0)\n",
    "        feature = mfccs\n",
    "        #[float(i) for i in feature]\n",
    "        #feature1=feature[:135]\n",
    "        df.loc[bookmark] = [feature]\n",
    "        bookmark=bookmark+1        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[-70.26777, -70.26777, -70.26777, -70.26777, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[-65.70765, -65.70765, -63.11472, -61.518997, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[-65.4825, -65.4825, -65.4825, -65.4825, -65.4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[-64.52845, -64.52845, -64.52845, -64.52845, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[-62.36431, -59.934727, -61.869602, -67.49577,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>955</th>\n",
       "      <td>[-31.163713, -28.617666, -24.917967, -24.49493...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>956</th>\n",
       "      <td>[-51.8394, -51.8394, -51.8394, -51.8394, -51.8...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>957</th>\n",
       "      <td>[-49.289055, -49.289055, -49.04858, -48.87291,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>958</th>\n",
       "      <td>[-57.722847, -57.722847, -57.722847, -57.72284...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>959</th>\n",
       "      <td>[-45.345825, -45.345825, -45.345825, -45.34582...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>960 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               feature\n",
       "0    [-70.26777, -70.26777, -70.26777, -70.26777, -...\n",
       "1    [-65.70765, -65.70765, -63.11472, -61.518997, ...\n",
       "2    [-65.4825, -65.4825, -65.4825, -65.4825, -65.4...\n",
       "3    [-64.52845, -64.52845, -64.52845, -64.52845, -...\n",
       "4    [-62.36431, -59.934727, -61.869602, -67.49577,...\n",
       "..                                                 ...\n",
       "955  [-31.163713, -28.617666, -24.917967, -24.49493...\n",
       "956  [-51.8394, -51.8394, -51.8394, -51.8394, -51.8...\n",
       "957  [-49.289055, -49.289055, -49.04858, -48.87291,...\n",
       "958  [-57.722847, -57.722847, -57.722847, -57.72284...\n",
       "959  [-45.345825, -45.345825, -45.345825, -45.34582...\n",
       "\n",
       "[960 rows x 1 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = pd.DataFrame(df['feature'].values.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df3[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "newdf = pd.concat([df3,labels], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnewdf = newdf.rename(index=str, columns={\"0\": \"label\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>207</th>\n",
       "      <th>208</th>\n",
       "      <th>209</th>\n",
       "      <th>210</th>\n",
       "      <th>211</th>\n",
       "      <th>212</th>\n",
       "      <th>213</th>\n",
       "      <th>214</th>\n",
       "      <th>215</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-70.267769</td>\n",
       "      <td>-70.267769</td>\n",
       "      <td>-70.267769</td>\n",
       "      <td>-70.267769</td>\n",
       "      <td>-70.267769</td>\n",
       "      <td>-70.267769</td>\n",
       "      <td>-70.267769</td>\n",
       "      <td>-70.267769</td>\n",
       "      <td>-70.267769</td>\n",
       "      <td>-70.267769</td>\n",
       "      <td>...</td>\n",
       "      <td>-57.447464</td>\n",
       "      <td>-58.896500</td>\n",
       "      <td>-58.750996</td>\n",
       "      <td>-57.405678</td>\n",
       "      <td>-60.078484</td>\n",
       "      <td>-63.426800</td>\n",
       "      <td>-62.638542</td>\n",
       "      <td>-61.082737</td>\n",
       "      <td>-60.234661</td>\n",
       "      <td>male_calm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-65.707649</td>\n",
       "      <td>-65.707649</td>\n",
       "      <td>-63.114719</td>\n",
       "      <td>-61.518997</td>\n",
       "      <td>-61.097141</td>\n",
       "      <td>-63.424599</td>\n",
       "      <td>-63.720066</td>\n",
       "      <td>-56.854614</td>\n",
       "      <td>-55.168972</td>\n",
       "      <td>-54.639999</td>\n",
       "      <td>...</td>\n",
       "      <td>-39.792141</td>\n",
       "      <td>-40.613159</td>\n",
       "      <td>-41.209202</td>\n",
       "      <td>-41.439201</td>\n",
       "      <td>-43.994278</td>\n",
       "      <td>-49.399620</td>\n",
       "      <td>-50.591599</td>\n",
       "      <td>-49.144051</td>\n",
       "      <td>-48.705654</td>\n",
       "      <td>female_calm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-65.482498</td>\n",
       "      <td>-65.482498</td>\n",
       "      <td>-65.482498</td>\n",
       "      <td>-65.482498</td>\n",
       "      <td>-65.482498</td>\n",
       "      <td>-65.482498</td>\n",
       "      <td>-65.482498</td>\n",
       "      <td>-65.482498</td>\n",
       "      <td>-65.482498</td>\n",
       "      <td>-65.482498</td>\n",
       "      <td>...</td>\n",
       "      <td>-31.346556</td>\n",
       "      <td>-34.310772</td>\n",
       "      <td>-35.800705</td>\n",
       "      <td>-35.936115</td>\n",
       "      <td>-37.631844</td>\n",
       "      <td>-40.119411</td>\n",
       "      <td>-41.662888</td>\n",
       "      <td>-41.323643</td>\n",
       "      <td>-40.710770</td>\n",
       "      <td>male_calm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-64.528450</td>\n",
       "      <td>-64.528450</td>\n",
       "      <td>-64.528450</td>\n",
       "      <td>-64.528450</td>\n",
       "      <td>-64.528450</td>\n",
       "      <td>-64.528450</td>\n",
       "      <td>-64.528450</td>\n",
       "      <td>-64.528450</td>\n",
       "      <td>-64.528450</td>\n",
       "      <td>-65.928223</td>\n",
       "      <td>...</td>\n",
       "      <td>-48.674301</td>\n",
       "      <td>-48.596073</td>\n",
       "      <td>-47.602745</td>\n",
       "      <td>-43.049198</td>\n",
       "      <td>-42.659542</td>\n",
       "      <td>-43.188560</td>\n",
       "      <td>-44.001240</td>\n",
       "      <td>-43.610100</td>\n",
       "      <td>-44.698246</td>\n",
       "      <td>female_calm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-62.364311</td>\n",
       "      <td>-59.934727</td>\n",
       "      <td>-61.869602</td>\n",
       "      <td>-67.495773</td>\n",
       "      <td>-71.071808</td>\n",
       "      <td>-65.679817</td>\n",
       "      <td>-63.394402</td>\n",
       "      <td>-65.503349</td>\n",
       "      <td>-61.856644</td>\n",
       "      <td>-60.005428</td>\n",
       "      <td>...</td>\n",
       "      <td>-39.071327</td>\n",
       "      <td>-41.897121</td>\n",
       "      <td>-40.865437</td>\n",
       "      <td>-38.290604</td>\n",
       "      <td>-36.372398</td>\n",
       "      <td>-37.915779</td>\n",
       "      <td>-40.026127</td>\n",
       "      <td>-43.383774</td>\n",
       "      <td>-43.965401</td>\n",
       "      <td>male_calm</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 217 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0          1          2          3          4          5    \\\n",
       "0 -70.267769 -70.267769 -70.267769 -70.267769 -70.267769 -70.267769   \n",
       "1 -65.707649 -65.707649 -63.114719 -61.518997 -61.097141 -63.424599   \n",
       "2 -65.482498 -65.482498 -65.482498 -65.482498 -65.482498 -65.482498   \n",
       "3 -64.528450 -64.528450 -64.528450 -64.528450 -64.528450 -64.528450   \n",
       "4 -62.364311 -59.934727 -61.869602 -67.495773 -71.071808 -65.679817   \n",
       "\n",
       "         6          7          8          9    ...        207        208  \\\n",
       "0 -70.267769 -70.267769 -70.267769 -70.267769  ... -57.447464 -58.896500   \n",
       "1 -63.720066 -56.854614 -55.168972 -54.639999  ... -39.792141 -40.613159   \n",
       "2 -65.482498 -65.482498 -65.482498 -65.482498  ... -31.346556 -34.310772   \n",
       "3 -64.528450 -64.528450 -64.528450 -65.928223  ... -48.674301 -48.596073   \n",
       "4 -63.394402 -65.503349 -61.856644 -60.005428  ... -39.071327 -41.897121   \n",
       "\n",
       "         209        210        211        212        213        214  \\\n",
       "0 -58.750996 -57.405678 -60.078484 -63.426800 -62.638542 -61.082737   \n",
       "1 -41.209202 -41.439201 -43.994278 -49.399620 -50.591599 -49.144051   \n",
       "2 -35.800705 -35.936115 -37.631844 -40.119411 -41.662888 -41.323643   \n",
       "3 -47.602745 -43.049198 -42.659542 -43.188560 -44.001240 -43.610100   \n",
       "4 -40.865437 -38.290604 -36.372398 -37.915779 -40.026127 -43.383774   \n",
       "\n",
       "         215          0    \n",
       "0 -60.234661    male_calm  \n",
       "1 -48.705654  female_calm  \n",
       "2 -40.710770    male_calm  \n",
       "3 -44.698246  female_calm  \n",
       "4 -43.965401    male_calm  \n",
       "\n",
       "[5 rows x 217 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnewdf[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>207</th>\n",
       "      <th>208</th>\n",
       "      <th>209</th>\n",
       "      <th>210</th>\n",
       "      <th>211</th>\n",
       "      <th>212</th>\n",
       "      <th>213</th>\n",
       "      <th>214</th>\n",
       "      <th>215</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>-68.377281</td>\n",
       "      <td>-71.174232</td>\n",
       "      <td>-72.227165</td>\n",
       "      <td>-70.870018</td>\n",
       "      <td>-69.319809</td>\n",
       "      <td>-64.230568</td>\n",
       "      <td>-65.108482</td>\n",
       "      <td>-64.682953</td>\n",
       "      <td>-62.713215</td>\n",
       "      <td>-59.147964</td>\n",
       "      <td>...</td>\n",
       "      <td>-36.386421</td>\n",
       "      <td>-39.448994</td>\n",
       "      <td>-40.346237</td>\n",
       "      <td>-34.800407</td>\n",
       "      <td>-32.871918</td>\n",
       "      <td>-32.259396</td>\n",
       "      <td>-32.719070</td>\n",
       "      <td>-34.919102</td>\n",
       "      <td>-36.199997</td>\n",
       "      <td>male_calm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>-53.745876</td>\n",
       "      <td>-52.634842</td>\n",
       "      <td>-53.519768</td>\n",
       "      <td>-54.932381</td>\n",
       "      <td>-55.329754</td>\n",
       "      <td>-54.357140</td>\n",
       "      <td>-54.874832</td>\n",
       "      <td>-55.920780</td>\n",
       "      <td>-52.162537</td>\n",
       "      <td>-50.480690</td>\n",
       "      <td>...</td>\n",
       "      <td>-36.638218</td>\n",
       "      <td>-38.412392</td>\n",
       "      <td>-37.785263</td>\n",
       "      <td>-39.638126</td>\n",
       "      <td>-42.478230</td>\n",
       "      <td>-42.426834</td>\n",
       "      <td>-43.424019</td>\n",
       "      <td>-43.171604</td>\n",
       "      <td>-41.451458</td>\n",
       "      <td>female_calm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>780</th>\n",
       "      <td>-61.363930</td>\n",
       "      <td>-60.424965</td>\n",
       "      <td>-60.542141</td>\n",
       "      <td>-63.737003</td>\n",
       "      <td>-64.673218</td>\n",
       "      <td>-61.401020</td>\n",
       "      <td>-62.801701</td>\n",
       "      <td>-64.522751</td>\n",
       "      <td>-64.676872</td>\n",
       "      <td>-63.780720</td>\n",
       "      <td>...</td>\n",
       "      <td>-58.870216</td>\n",
       "      <td>-61.683640</td>\n",
       "      <td>-59.392166</td>\n",
       "      <td>-58.266258</td>\n",
       "      <td>-60.922646</td>\n",
       "      <td>-64.676872</td>\n",
       "      <td>-64.676872</td>\n",
       "      <td>-64.676872</td>\n",
       "      <td>-64.178909</td>\n",
       "      <td>male_fearful</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>-53.552849</td>\n",
       "      <td>-52.381294</td>\n",
       "      <td>-49.825081</td>\n",
       "      <td>-50.466629</td>\n",
       "      <td>-54.859333</td>\n",
       "      <td>-57.549725</td>\n",
       "      <td>-59.093304</td>\n",
       "      <td>-57.289921</td>\n",
       "      <td>-57.218040</td>\n",
       "      <td>-58.383255</td>\n",
       "      <td>...</td>\n",
       "      <td>-50.852089</td>\n",
       "      <td>-52.688179</td>\n",
       "      <td>-54.263847</td>\n",
       "      <td>-53.873901</td>\n",
       "      <td>-57.603317</td>\n",
       "      <td>-59.374004</td>\n",
       "      <td>-58.332535</td>\n",
       "      <td>-57.452694</td>\n",
       "      <td>-58.009605</td>\n",
       "      <td>female_calm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>-63.131767</td>\n",
       "      <td>-61.888165</td>\n",
       "      <td>-59.611084</td>\n",
       "      <td>-57.938656</td>\n",
       "      <td>-61.089252</td>\n",
       "      <td>-60.685398</td>\n",
       "      <td>-62.597221</td>\n",
       "      <td>-68.623726</td>\n",
       "      <td>-69.007233</td>\n",
       "      <td>-66.853775</td>\n",
       "      <td>...</td>\n",
       "      <td>-63.206009</td>\n",
       "      <td>-60.798763</td>\n",
       "      <td>-60.709068</td>\n",
       "      <td>-56.783298</td>\n",
       "      <td>-55.975407</td>\n",
       "      <td>-61.550983</td>\n",
       "      <td>-63.805748</td>\n",
       "      <td>-60.823490</td>\n",
       "      <td>-57.963692</td>\n",
       "      <td>male_calm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>438</th>\n",
       "      <td>-71.805733</td>\n",
       "      <td>-71.805733</td>\n",
       "      <td>-71.805733</td>\n",
       "      <td>-71.805733</td>\n",
       "      <td>-71.805733</td>\n",
       "      <td>-71.805733</td>\n",
       "      <td>-71.805733</td>\n",
       "      <td>-71.805733</td>\n",
       "      <td>-71.805733</td>\n",
       "      <td>-71.805733</td>\n",
       "      <td>...</td>\n",
       "      <td>-48.080452</td>\n",
       "      <td>-48.708305</td>\n",
       "      <td>-47.572922</td>\n",
       "      <td>-48.468567</td>\n",
       "      <td>-48.897652</td>\n",
       "      <td>-50.247070</td>\n",
       "      <td>-49.038086</td>\n",
       "      <td>-48.715557</td>\n",
       "      <td>-51.587341</td>\n",
       "      <td>male_sad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>507</th>\n",
       "      <td>-49.085495</td>\n",
       "      <td>-49.085495</td>\n",
       "      <td>-49.085495</td>\n",
       "      <td>-49.085495</td>\n",
       "      <td>-49.085495</td>\n",
       "      <td>-49.085495</td>\n",
       "      <td>-49.085495</td>\n",
       "      <td>-49.085495</td>\n",
       "      <td>-49.085495</td>\n",
       "      <td>-49.085495</td>\n",
       "      <td>...</td>\n",
       "      <td>-47.262310</td>\n",
       "      <td>-46.226849</td>\n",
       "      <td>-45.134892</td>\n",
       "      <td>-47.326176</td>\n",
       "      <td>-49.049461</td>\n",
       "      <td>-49.085495</td>\n",
       "      <td>-48.773647</td>\n",
       "      <td>-47.609108</td>\n",
       "      <td>-48.342606</td>\n",
       "      <td>female_sad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>-49.250637</td>\n",
       "      <td>-47.740860</td>\n",
       "      <td>-49.672993</td>\n",
       "      <td>-50.764851</td>\n",
       "      <td>-51.801365</td>\n",
       "      <td>-51.357605</td>\n",
       "      <td>-51.992981</td>\n",
       "      <td>-51.296574</td>\n",
       "      <td>-49.914944</td>\n",
       "      <td>-51.691490</td>\n",
       "      <td>...</td>\n",
       "      <td>-45.861893</td>\n",
       "      <td>-46.302067</td>\n",
       "      <td>-49.884659</td>\n",
       "      <td>-48.919327</td>\n",
       "      <td>-49.577133</td>\n",
       "      <td>-49.001667</td>\n",
       "      <td>-47.466061</td>\n",
       "      <td>-47.603310</td>\n",
       "      <td>-46.976017</td>\n",
       "      <td>male_calm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>749</th>\n",
       "      <td>-40.054913</td>\n",
       "      <td>-40.072048</td>\n",
       "      <td>-40.383789</td>\n",
       "      <td>-40.833176</td>\n",
       "      <td>-40.328072</td>\n",
       "      <td>-39.382156</td>\n",
       "      <td>-39.575920</td>\n",
       "      <td>-39.638176</td>\n",
       "      <td>-39.379601</td>\n",
       "      <td>-39.370834</td>\n",
       "      <td>...</td>\n",
       "      <td>-35.521240</td>\n",
       "      <td>-33.976696</td>\n",
       "      <td>-32.924881</td>\n",
       "      <td>-32.429649</td>\n",
       "      <td>-32.133648</td>\n",
       "      <td>-30.997250</td>\n",
       "      <td>-33.306576</td>\n",
       "      <td>-34.658836</td>\n",
       "      <td>-31.983351</td>\n",
       "      <td>female_angry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>-62.610722</td>\n",
       "      <td>-62.610722</td>\n",
       "      <td>-62.610722</td>\n",
       "      <td>-62.610722</td>\n",
       "      <td>-62.610722</td>\n",
       "      <td>-62.610722</td>\n",
       "      <td>-62.610722</td>\n",
       "      <td>-62.610722</td>\n",
       "      <td>-62.610722</td>\n",
       "      <td>-62.610722</td>\n",
       "      <td>...</td>\n",
       "      <td>-40.386753</td>\n",
       "      <td>-42.516563</td>\n",
       "      <td>-41.397415</td>\n",
       "      <td>-41.803909</td>\n",
       "      <td>-41.774162</td>\n",
       "      <td>-40.359371</td>\n",
       "      <td>-41.822468</td>\n",
       "      <td>-42.906578</td>\n",
       "      <td>-44.800964</td>\n",
       "      <td>female_calm</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 217 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0          1          2          3          4          5    \\\n",
       "124 -68.377281 -71.174232 -72.227165 -70.870018 -69.319809 -64.230568   \n",
       "177 -53.745876 -52.634842 -53.519768 -54.932381 -55.329754 -54.357140   \n",
       "780 -61.363930 -60.424965 -60.542141 -63.737003 -64.673218 -61.401020   \n",
       "83  -53.552849 -52.381294 -49.825081 -50.466629 -54.859333 -57.549725   \n",
       "12  -63.131767 -61.888165 -59.611084 -57.938656 -61.089252 -60.685398   \n",
       "438 -71.805733 -71.805733 -71.805733 -71.805733 -71.805733 -71.805733   \n",
       "507 -49.085495 -49.085495 -49.085495 -49.085495 -49.085495 -49.085495   \n",
       "64  -49.250637 -47.740860 -49.672993 -50.764851 -51.801365 -51.357605   \n",
       "749 -40.054913 -40.072048 -40.383789 -40.833176 -40.328072 -39.382156   \n",
       "39  -62.610722 -62.610722 -62.610722 -62.610722 -62.610722 -62.610722   \n",
       "\n",
       "           6          7          8          9    ...        207        208  \\\n",
       "124 -65.108482 -64.682953 -62.713215 -59.147964  ... -36.386421 -39.448994   \n",
       "177 -54.874832 -55.920780 -52.162537 -50.480690  ... -36.638218 -38.412392   \n",
       "780 -62.801701 -64.522751 -64.676872 -63.780720  ... -58.870216 -61.683640   \n",
       "83  -59.093304 -57.289921 -57.218040 -58.383255  ... -50.852089 -52.688179   \n",
       "12  -62.597221 -68.623726 -69.007233 -66.853775  ... -63.206009 -60.798763   \n",
       "438 -71.805733 -71.805733 -71.805733 -71.805733  ... -48.080452 -48.708305   \n",
       "507 -49.085495 -49.085495 -49.085495 -49.085495  ... -47.262310 -46.226849   \n",
       "64  -51.992981 -51.296574 -49.914944 -51.691490  ... -45.861893 -46.302067   \n",
       "749 -39.575920 -39.638176 -39.379601 -39.370834  ... -35.521240 -33.976696   \n",
       "39  -62.610722 -62.610722 -62.610722 -62.610722  ... -40.386753 -42.516563   \n",
       "\n",
       "           209        210        211        212        213        214  \\\n",
       "124 -40.346237 -34.800407 -32.871918 -32.259396 -32.719070 -34.919102   \n",
       "177 -37.785263 -39.638126 -42.478230 -42.426834 -43.424019 -43.171604   \n",
       "780 -59.392166 -58.266258 -60.922646 -64.676872 -64.676872 -64.676872   \n",
       "83  -54.263847 -53.873901 -57.603317 -59.374004 -58.332535 -57.452694   \n",
       "12  -60.709068 -56.783298 -55.975407 -61.550983 -63.805748 -60.823490   \n",
       "438 -47.572922 -48.468567 -48.897652 -50.247070 -49.038086 -48.715557   \n",
       "507 -45.134892 -47.326176 -49.049461 -49.085495 -48.773647 -47.609108   \n",
       "64  -49.884659 -48.919327 -49.577133 -49.001667 -47.466061 -47.603310   \n",
       "749 -32.924881 -32.429649 -32.133648 -30.997250 -33.306576 -34.658836   \n",
       "39  -41.397415 -41.803909 -41.774162 -40.359371 -41.822468 -42.906578   \n",
       "\n",
       "           215           0    \n",
       "124 -36.199997     male_calm  \n",
       "177 -41.451458   female_calm  \n",
       "780 -64.178909  male_fearful  \n",
       "83  -58.009605   female_calm  \n",
       "12  -57.963692     male_calm  \n",
       "438 -51.587341      male_sad  \n",
       "507 -48.342606    female_sad  \n",
       "64  -46.976017     male_calm  \n",
       "749 -31.983351  female_angry  \n",
       "39  -44.800964   female_calm  \n",
       "\n",
       "[10 rows x 217 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.utils import shuffle\n",
    "rnewdf = shuffle(newdf)\n",
    "rnewdf[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnewdf=rnewdf.fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dividing the data into test and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "newdf1 = np.random.rand(len(rnewdf)) < 0.8\n",
    "train = rnewdf[newdf1]\n",
    "test = rnewdf[~newdf1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>207</th>\n",
       "      <th>208</th>\n",
       "      <th>209</th>\n",
       "      <th>210</th>\n",
       "      <th>211</th>\n",
       "      <th>212</th>\n",
       "      <th>213</th>\n",
       "      <th>214</th>\n",
       "      <th>215</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>917</th>\n",
       "      <td>-40.386227</td>\n",
       "      <td>-40.550884</td>\n",
       "      <td>-39.979263</td>\n",
       "      <td>-38.504513</td>\n",
       "      <td>-39.087074</td>\n",
       "      <td>-39.799557</td>\n",
       "      <td>-40.708920</td>\n",
       "      <td>-40.157829</td>\n",
       "      <td>-40.237572</td>\n",
       "      <td>-41.582844</td>\n",
       "      <td>...</td>\n",
       "      <td>-38.580383</td>\n",
       "      <td>-37.227379</td>\n",
       "      <td>-37.496994</td>\n",
       "      <td>-37.820793</td>\n",
       "      <td>-38.419064</td>\n",
       "      <td>-40.801216</td>\n",
       "      <td>-40.612724</td>\n",
       "      <td>-40.212498</td>\n",
       "      <td>-38.062607</td>\n",
       "      <td>female_fearful</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-73.030594</td>\n",
       "      <td>-72.721794</td>\n",
       "      <td>-72.499947</td>\n",
       "      <td>-73.626694</td>\n",
       "      <td>-73.626694</td>\n",
       "      <td>-73.626694</td>\n",
       "      <td>-73.626694</td>\n",
       "      <td>-73.626694</td>\n",
       "      <td>-73.626694</td>\n",
       "      <td>-73.626694</td>\n",
       "      <td>...</td>\n",
       "      <td>-56.701408</td>\n",
       "      <td>-55.808964</td>\n",
       "      <td>-56.925110</td>\n",
       "      <td>-57.031487</td>\n",
       "      <td>-58.843853</td>\n",
       "      <td>-58.682259</td>\n",
       "      <td>-57.785545</td>\n",
       "      <td>-59.132893</td>\n",
       "      <td>-62.051418</td>\n",
       "      <td>male_calm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>405</th>\n",
       "      <td>-59.255074</td>\n",
       "      <td>-59.218208</td>\n",
       "      <td>-61.168941</td>\n",
       "      <td>-59.712498</td>\n",
       "      <td>-59.700230</td>\n",
       "      <td>-60.960548</td>\n",
       "      <td>-60.868977</td>\n",
       "      <td>-59.021080</td>\n",
       "      <td>-59.191807</td>\n",
       "      <td>-61.754379</td>\n",
       "      <td>...</td>\n",
       "      <td>-52.883572</td>\n",
       "      <td>-52.793884</td>\n",
       "      <td>-55.290218</td>\n",
       "      <td>-57.921272</td>\n",
       "      <td>-56.429016</td>\n",
       "      <td>-57.002102</td>\n",
       "      <td>-58.053421</td>\n",
       "      <td>-58.613388</td>\n",
       "      <td>-56.979038</td>\n",
       "      <td>female_sad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>-69.243256</td>\n",
       "      <td>-69.243256</td>\n",
       "      <td>-69.243256</td>\n",
       "      <td>-69.243256</td>\n",
       "      <td>-68.901970</td>\n",
       "      <td>-67.983002</td>\n",
       "      <td>-68.089203</td>\n",
       "      <td>-67.897331</td>\n",
       "      <td>-65.258011</td>\n",
       "      <td>-67.170975</td>\n",
       "      <td>...</td>\n",
       "      <td>-29.354748</td>\n",
       "      <td>-30.388714</td>\n",
       "      <td>-30.094086</td>\n",
       "      <td>-30.615465</td>\n",
       "      <td>-31.126341</td>\n",
       "      <td>-31.148714</td>\n",
       "      <td>-31.413185</td>\n",
       "      <td>-31.356203</td>\n",
       "      <td>-30.060383</td>\n",
       "      <td>male_calm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>490</th>\n",
       "      <td>-54.744747</td>\n",
       "      <td>-54.456314</td>\n",
       "      <td>-56.971970</td>\n",
       "      <td>-59.304497</td>\n",
       "      <td>-59.304497</td>\n",
       "      <td>-59.304497</td>\n",
       "      <td>-58.027264</td>\n",
       "      <td>-56.805405</td>\n",
       "      <td>-54.136921</td>\n",
       "      <td>-54.495789</td>\n",
       "      <td>...</td>\n",
       "      <td>-53.959877</td>\n",
       "      <td>-51.744961</td>\n",
       "      <td>-52.448994</td>\n",
       "      <td>-51.844112</td>\n",
       "      <td>-51.704891</td>\n",
       "      <td>-51.505814</td>\n",
       "      <td>-52.127766</td>\n",
       "      <td>-53.147602</td>\n",
       "      <td>-52.696941</td>\n",
       "      <td>male_sad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>948</th>\n",
       "      <td>-63.898258</td>\n",
       "      <td>-59.659367</td>\n",
       "      <td>-56.782394</td>\n",
       "      <td>-54.998428</td>\n",
       "      <td>-58.540775</td>\n",
       "      <td>-63.898258</td>\n",
       "      <td>-61.179317</td>\n",
       "      <td>-58.758701</td>\n",
       "      <td>-61.959270</td>\n",
       "      <td>-63.536823</td>\n",
       "      <td>...</td>\n",
       "      <td>-56.474083</td>\n",
       "      <td>-58.214920</td>\n",
       "      <td>-58.713482</td>\n",
       "      <td>-58.827705</td>\n",
       "      <td>-57.919724</td>\n",
       "      <td>-57.178616</td>\n",
       "      <td>-57.815914</td>\n",
       "      <td>-62.274448</td>\n",
       "      <td>-63.898258</td>\n",
       "      <td>male_fearful</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>-53.145382</td>\n",
       "      <td>-51.780399</td>\n",
       "      <td>-50.108887</td>\n",
       "      <td>-50.054245</td>\n",
       "      <td>-50.522495</td>\n",
       "      <td>-49.809898</td>\n",
       "      <td>-49.573807</td>\n",
       "      <td>-50.368843</td>\n",
       "      <td>-49.523979</td>\n",
       "      <td>-49.933105</td>\n",
       "      <td>...</td>\n",
       "      <td>-25.786306</td>\n",
       "      <td>-23.449816</td>\n",
       "      <td>-23.891396</td>\n",
       "      <td>-23.845661</td>\n",
       "      <td>-23.681883</td>\n",
       "      <td>-24.520052</td>\n",
       "      <td>-24.730570</td>\n",
       "      <td>-26.018826</td>\n",
       "      <td>-24.973238</td>\n",
       "      <td>male_sad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>631</th>\n",
       "      <td>-50.773075</td>\n",
       "      <td>-51.290535</td>\n",
       "      <td>-49.537144</td>\n",
       "      <td>-49.165077</td>\n",
       "      <td>-49.960850</td>\n",
       "      <td>-49.895115</td>\n",
       "      <td>-50.056541</td>\n",
       "      <td>-47.998516</td>\n",
       "      <td>-49.212078</td>\n",
       "      <td>-51.043209</td>\n",
       "      <td>...</td>\n",
       "      <td>-51.971634</td>\n",
       "      <td>-51.041477</td>\n",
       "      <td>-49.913498</td>\n",
       "      <td>-51.230728</td>\n",
       "      <td>-52.506657</td>\n",
       "      <td>-50.665371</td>\n",
       "      <td>-49.479511</td>\n",
       "      <td>-50.153622</td>\n",
       "      <td>-50.676296</td>\n",
       "      <td>female_angry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>477</th>\n",
       "      <td>-52.832333</td>\n",
       "      <td>-52.688438</td>\n",
       "      <td>-52.211685</td>\n",
       "      <td>-51.576759</td>\n",
       "      <td>-51.757797</td>\n",
       "      <td>-51.686863</td>\n",
       "      <td>-52.709007</td>\n",
       "      <td>-53.476120</td>\n",
       "      <td>-54.586895</td>\n",
       "      <td>-56.060478</td>\n",
       "      <td>...</td>\n",
       "      <td>-51.755314</td>\n",
       "      <td>-52.064899</td>\n",
       "      <td>-49.577896</td>\n",
       "      <td>-49.165901</td>\n",
       "      <td>-49.457909</td>\n",
       "      <td>-51.753361</td>\n",
       "      <td>-53.608810</td>\n",
       "      <td>-53.711082</td>\n",
       "      <td>-52.081238</td>\n",
       "      <td>female_sad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>446</th>\n",
       "      <td>-50.834782</td>\n",
       "      <td>-50.984879</td>\n",
       "      <td>-53.106361</td>\n",
       "      <td>-51.711849</td>\n",
       "      <td>-53.195004</td>\n",
       "      <td>-53.064438</td>\n",
       "      <td>-52.546829</td>\n",
       "      <td>-53.192570</td>\n",
       "      <td>-51.704960</td>\n",
       "      <td>-52.067913</td>\n",
       "      <td>...</td>\n",
       "      <td>-53.111538</td>\n",
       "      <td>-53.988560</td>\n",
       "      <td>-52.939934</td>\n",
       "      <td>-52.597675</td>\n",
       "      <td>-52.951694</td>\n",
       "      <td>-52.803474</td>\n",
       "      <td>-53.736084</td>\n",
       "      <td>-53.410015</td>\n",
       "      <td>-52.198318</td>\n",
       "      <td>male_sad</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 217 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0          1          2          3          4          5    \\\n",
       "917 -40.386227 -40.550884 -39.979263 -38.504513 -39.087074 -39.799557   \n",
       "8   -73.030594 -72.721794 -72.499947 -73.626694 -73.626694 -73.626694   \n",
       "405 -59.255074 -59.218208 -61.168941 -59.712498 -59.700230 -60.960548   \n",
       "120 -69.243256 -69.243256 -69.243256 -69.243256 -68.901970 -67.983002   \n",
       "490 -54.744747 -54.456314 -56.971970 -59.304497 -59.304497 -59.304497   \n",
       "948 -63.898258 -59.659367 -56.782394 -54.998428 -58.540775 -63.898258   \n",
       "498 -53.145382 -51.780399 -50.108887 -50.054245 -50.522495 -49.809898   \n",
       "631 -50.773075 -51.290535 -49.537144 -49.165077 -49.960850 -49.895115   \n",
       "477 -52.832333 -52.688438 -52.211685 -51.576759 -51.757797 -51.686863   \n",
       "446 -50.834782 -50.984879 -53.106361 -51.711849 -53.195004 -53.064438   \n",
       "\n",
       "           6          7          8          9    ...        207        208  \\\n",
       "917 -40.708920 -40.157829 -40.237572 -41.582844  ... -38.580383 -37.227379   \n",
       "8   -73.626694 -73.626694 -73.626694 -73.626694  ... -56.701408 -55.808964   \n",
       "405 -60.868977 -59.021080 -59.191807 -61.754379  ... -52.883572 -52.793884   \n",
       "120 -68.089203 -67.897331 -65.258011 -67.170975  ... -29.354748 -30.388714   \n",
       "490 -58.027264 -56.805405 -54.136921 -54.495789  ... -53.959877 -51.744961   \n",
       "948 -61.179317 -58.758701 -61.959270 -63.536823  ... -56.474083 -58.214920   \n",
       "498 -49.573807 -50.368843 -49.523979 -49.933105  ... -25.786306 -23.449816   \n",
       "631 -50.056541 -47.998516 -49.212078 -51.043209  ... -51.971634 -51.041477   \n",
       "477 -52.709007 -53.476120 -54.586895 -56.060478  ... -51.755314 -52.064899   \n",
       "446 -52.546829 -53.192570 -51.704960 -52.067913  ... -53.111538 -53.988560   \n",
       "\n",
       "           209        210        211        212        213        214  \\\n",
       "917 -37.496994 -37.820793 -38.419064 -40.801216 -40.612724 -40.212498   \n",
       "8   -56.925110 -57.031487 -58.843853 -58.682259 -57.785545 -59.132893   \n",
       "405 -55.290218 -57.921272 -56.429016 -57.002102 -58.053421 -58.613388   \n",
       "120 -30.094086 -30.615465 -31.126341 -31.148714 -31.413185 -31.356203   \n",
       "490 -52.448994 -51.844112 -51.704891 -51.505814 -52.127766 -53.147602   \n",
       "948 -58.713482 -58.827705 -57.919724 -57.178616 -57.815914 -62.274448   \n",
       "498 -23.891396 -23.845661 -23.681883 -24.520052 -24.730570 -26.018826   \n",
       "631 -49.913498 -51.230728 -52.506657 -50.665371 -49.479511 -50.153622   \n",
       "477 -49.577896 -49.165901 -49.457909 -51.753361 -53.608810 -53.711082   \n",
       "446 -52.939934 -52.597675 -52.951694 -52.803474 -53.736084 -53.410015   \n",
       "\n",
       "           215             0    \n",
       "917 -38.062607  female_fearful  \n",
       "8   -62.051418       male_calm  \n",
       "405 -56.979038      female_sad  \n",
       "120 -30.060383       male_calm  \n",
       "490 -52.696941        male_sad  \n",
       "948 -63.898258    male_fearful  \n",
       "498 -24.973238        male_sad  \n",
       "631 -50.676296    female_angry  \n",
       "477 -52.081238      female_sad  \n",
       "446 -52.198318        male_sad  \n",
       "\n",
       "[10 rows x 217 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[250:260]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainfeatures = train.iloc[:, :-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainlabel = train.iloc[:, -1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "testfeatures = test.iloc[:, :-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "testlabel = test.iloc[:, -1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\reddyvs\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\utils\\validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import np_utils\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "X_train = np.array(trainfeatures)\n",
    "y_train = np.array(trainlabel)\n",
    "X_test = np.array(testfeatures)\n",
    "y_test = np.array(testlabel)\n",
    "\n",
    "lb = LabelEncoder()\n",
    "\n",
    "y_train = np_utils.to_categorical(lb.fit_transform(y_train))\n",
    "y_test = np_utils.to_categorical(lb.fit_transform(y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 1., 0., 0.],\n",
       "       [0., 1., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 1., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 1., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(771, 216)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Changing dimension for CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x_traincnn =np.expand_dims(X_train, axis=2)\n",
    "x_testcnn= np.expand_dims(X_test, axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv1D(256, 5,padding='same',\n",
    "                 input_shape=(216,1)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv1D(128, 5,padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(MaxPooling1D(pool_size=(8)))\n",
    "model.add(Conv1D(128, 5,padding='same',))\n",
    "model.add(Activation('relu'))\n",
    "#model.add(Conv1D(128, 5,padding='same',))\n",
    "#model.add(Activation('relu'))\n",
    "#model.add(Conv1D(128, 5,padding='same',))\n",
    "#model.add(Activation('relu'))\n",
    "#model.add(Dropout(0.2))\n",
    "model.add(Conv1D(128, 5,padding='same',))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(10))\n",
    "model.add(Activation('softmax'))\n",
    "opt = keras.optimizers.rmsprop(lr=0.00001, decay=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_1 (Conv1D)            (None, 216, 256)          1536      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 216, 256)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 216, 128)          163968    \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 216, 128)          0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 216, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 27, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 27, 128)           82048     \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 27, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 27, 128)           82048     \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 27, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 3456)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                34570     \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 364,170\n",
      "Trainable params: 364,170\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer=opt,metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removed the whole training part for avoiding unnecessary long epochs list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 771 samples, validate on 189 samples\n",
      "Epoch 1/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 2.9009 - accuracy: 0.1154 - val_loss: 2.3122 - val_accuracy: 0.1799\n",
      "Epoch 2/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 2.2890 - accuracy: 0.1569 - val_loss: 2.2616 - val_accuracy: 0.1587\n",
      "Epoch 3/700\n",
      "771/771 [==============================] - 3s 4ms/step - loss: 2.2333 - accuracy: 0.1699 - val_loss: 2.2448 - val_accuracy: 0.1799\n",
      "Epoch 4/700\n",
      "771/771 [==============================] - 3s 4ms/step - loss: 2.1965 - accuracy: 0.1699 - val_loss: 2.2424 - val_accuracy: 0.1111\n",
      "Epoch 5/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 2.1673 - accuracy: 0.1790 - val_loss: 2.1937 - val_accuracy: 0.2011\n",
      "Epoch 6/700\n",
      "771/771 [==============================] - 3s 4ms/step - loss: 2.1501 - accuracy: 0.1920 - val_loss: 2.1842 - val_accuracy: 0.2063\n",
      "Epoch 7/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 2.1274 - accuracy: 0.2114 - val_loss: 2.1916 - val_accuracy: 0.1217\n",
      "Epoch 8/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 2.1014 - accuracy: 0.2387 - val_loss: 2.1585 - val_accuracy: 0.1746\n",
      "Epoch 9/700\n",
      "771/771 [==============================] - 3s 5ms/step - loss: 2.0799 - accuracy: 0.2542 - val_loss: 2.1449 - val_accuracy: 0.1905\n",
      "Epoch 10/700\n",
      "771/771 [==============================] - 3s 4ms/step - loss: 2.0644 - accuracy: 0.2516 - val_loss: 2.1433 - val_accuracy: 0.2011\n",
      "Epoch 11/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 2.0456 - accuracy: 0.2568 - val_loss: 2.1315 - val_accuracy: 0.2381\n",
      "Epoch 12/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 2.0223 - accuracy: 0.2815 - val_loss: 2.0883 - val_accuracy: 0.1799\n",
      "Epoch 13/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 2.0024 - accuracy: 0.2646 - val_loss: 2.0755 - val_accuracy: 0.2804\n",
      "Epoch 14/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 1.9818 - accuracy: 0.2853 - val_loss: 2.0884 - val_accuracy: 0.2593\n",
      "Epoch 15/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 1.9680 - accuracy: 0.3268 - val_loss: 2.0714 - val_accuracy: 0.2646\n",
      "Epoch 16/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 1.9373 - accuracy: 0.3100 - val_loss: 2.0440 - val_accuracy: 0.2593\n",
      "Epoch 17/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 1.9285 - accuracy: 0.3256 - val_loss: 2.0531 - val_accuracy: 0.2593\n",
      "Epoch 18/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 1.9115 - accuracy: 0.3165 - val_loss: 2.0044 - val_accuracy: 0.2593\n",
      "Epoch 19/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 1.8888 - accuracy: 0.3372 - val_loss: 2.0530 - val_accuracy: 0.2487\n",
      "Epoch 20/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 1.8860 - accuracy: 0.3307 - val_loss: 2.0181 - val_accuracy: 0.2646\n",
      "Epoch 21/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 1.8608 - accuracy: 0.3268 - val_loss: 2.0640 - val_accuracy: 0.2063\n",
      "Epoch 22/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 1.8577 - accuracy: 0.3411 - val_loss: 1.9828 - val_accuracy: 0.2646\n",
      "Epoch 23/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 1.8422 - accuracy: 0.3204 - val_loss: 1.9859 - val_accuracy: 0.2804\n",
      "Epoch 24/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 1.8210 - accuracy: 0.3476 - val_loss: 1.9589 - val_accuracy: 0.3122\n",
      "Epoch 25/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 1.8122 - accuracy: 0.3593 - val_loss: 1.9385 - val_accuracy: 0.2698\n",
      "Epoch 26/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 1.8034 - accuracy: 0.3645 - val_loss: 1.9568 - val_accuracy: 0.2804\n",
      "Epoch 27/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 1.7826 - accuracy: 0.3619 - val_loss: 1.9786 - val_accuracy: 0.2910\n",
      "Epoch 28/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 1.7706 - accuracy: 0.3865 - val_loss: 1.9120 - val_accuracy: 0.3175\n",
      "Epoch 29/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 1.7574 - accuracy: 0.3696 - val_loss: 1.9174 - val_accuracy: 0.2593\n",
      "Epoch 30/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 1.7512 - accuracy: 0.3593 - val_loss: 1.9490 - val_accuracy: 0.2593\n",
      "Epoch 31/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 1.7397 - accuracy: 0.3774 - val_loss: 1.9196 - val_accuracy: 0.2963\n",
      "Epoch 32/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 1.7263 - accuracy: 0.3787 - val_loss: 1.8863 - val_accuracy: 0.3228\n",
      "Epoch 33/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 1.7122 - accuracy: 0.3943 - val_loss: 1.8805 - val_accuracy: 0.3280\n",
      "Epoch 34/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 1.7134 - accuracy: 0.3748 - val_loss: 1.8655 - val_accuracy: 0.3280\n",
      "Epoch 35/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 1.6946 - accuracy: 0.3891 - val_loss: 1.8937 - val_accuracy: 0.2857\n",
      "Epoch 36/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 1.6924 - accuracy: 0.3722 - val_loss: 1.8581 - val_accuracy: 0.3228\n",
      "Epoch 37/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 1.6718 - accuracy: 0.4008 - val_loss: 1.8737 - val_accuracy: 0.3280\n",
      "Epoch 38/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 1.6541 - accuracy: 0.4047 - val_loss: 1.8673 - val_accuracy: 0.2593\n",
      "Epoch 39/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 1.6673 - accuracy: 0.3917 - val_loss: 1.8410 - val_accuracy: 0.3069\n",
      "Epoch 40/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 1.6414 - accuracy: 0.4099 - val_loss: 1.9219 - val_accuracy: 0.2910\n",
      "Epoch 41/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 1.6405 - accuracy: 0.4047 - val_loss: 1.8281 - val_accuracy: 0.3386\n",
      "Epoch 42/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 1.6273 - accuracy: 0.4293 - val_loss: 1.8371 - val_accuracy: 0.2963\n",
      "Epoch 43/700\n",
      "771/771 [==============================] - 3s 4ms/step - loss: 1.6170 - accuracy: 0.4176 - val_loss: 1.8215 - val_accuracy: 0.3280\n",
      "Epoch 44/700\n",
      "771/771 [==============================] - 3s 4ms/step - loss: 1.6101 - accuracy: 0.4137 - val_loss: 1.8504 - val_accuracy: 0.3016\n",
      "Epoch 45/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 1.5955 - accuracy: 0.4241 - val_loss: 1.8229 - val_accuracy: 0.3651\n",
      "Epoch 46/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 1.5911 - accuracy: 0.4189 - val_loss: 1.7905 - val_accuracy: 0.3386\n",
      "Epoch 47/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 1.5900 - accuracy: 0.4202 - val_loss: 1.7751 - val_accuracy: 0.3862\n",
      "Epoch 48/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 1.5790 - accuracy: 0.4215 - val_loss: 1.7794 - val_accuracy: 0.3810\n",
      "Epoch 49/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 1.5621 - accuracy: 0.4410 - val_loss: 1.8142 - val_accuracy: 0.3122\n",
      "Epoch 50/700\n",
      "771/771 [==============================] - 3s 4ms/step - loss: 1.5634 - accuracy: 0.4449 - val_loss: 1.7691 - val_accuracy: 0.3333\n",
      "Epoch 51/700\n",
      "771/771 [==============================] - 3s 4ms/step - loss: 1.5516 - accuracy: 0.4176 - val_loss: 1.8501 - val_accuracy: 0.2804\n",
      "Epoch 52/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 1.5537 - accuracy: 0.4397 - val_loss: 1.7459 - val_accuracy: 0.3386\n",
      "Epoch 53/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 1.5463 - accuracy: 0.4280 - val_loss: 1.8355 - val_accuracy: 0.3016\n",
      "Epoch 54/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 1.5236 - accuracy: 0.4488 - val_loss: 1.7644 - val_accuracy: 0.3386\n",
      "Epoch 55/700\n",
      "771/771 [==============================] - 3s 4ms/step - loss: 1.5259 - accuracy: 0.4397 - val_loss: 1.7857 - val_accuracy: 0.3598\n",
      "Epoch 56/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 1.5128 - accuracy: 0.4527 - val_loss: 1.8330 - val_accuracy: 0.2910\n",
      "Epoch 57/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 1.5127 - accuracy: 0.4449 - val_loss: 1.7833 - val_accuracy: 0.3280\n",
      "Epoch 58/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 1.5013 - accuracy: 0.4358 - val_loss: 1.7501 - val_accuracy: 0.3439\n",
      "Epoch 59/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 1.4955 - accuracy: 0.4604 - val_loss: 1.6972 - val_accuracy: 0.4180\n",
      "Epoch 60/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 1.4761 - accuracy: 0.4656 - val_loss: 1.7787 - val_accuracy: 0.3492\n",
      "Epoch 61/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 1.4911 - accuracy: 0.4630 - val_loss: 1.7142 - val_accuracy: 0.3862\n",
      "Epoch 62/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 1.4719 - accuracy: 0.4591 - val_loss: 1.7015 - val_accuracy: 0.3862\n",
      "Epoch 63/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 1.4689 - accuracy: 0.4617 - val_loss: 1.6818 - val_accuracy: 0.3757\n",
      "Epoch 64/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 1.4578 - accuracy: 0.4773 - val_loss: 1.6736 - val_accuracy: 0.3915\n",
      "Epoch 65/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 1.4522 - accuracy: 0.4604 - val_loss: 1.7332 - val_accuracy: 0.3386\n",
      "Epoch 66/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 1.4488 - accuracy: 0.4617 - val_loss: 1.7984 - val_accuracy: 0.3439\n",
      "Epoch 67/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 1.4399 - accuracy: 0.4695 - val_loss: 1.7460 - val_accuracy: 0.3069\n",
      "Epoch 68/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 1.4285 - accuracy: 0.4773 - val_loss: 1.6883 - val_accuracy: 0.3333\n",
      "Epoch 69/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 1.4329 - accuracy: 0.4643 - val_loss: 1.6848 - val_accuracy: 0.3810\n",
      "Epoch 70/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 1.4235 - accuracy: 0.4669 - val_loss: 1.7394 - val_accuracy: 0.3439\n",
      "Epoch 71/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 1.4211 - accuracy: 0.4630 - val_loss: 1.6951 - val_accuracy: 0.3598\n",
      "Epoch 72/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 1.4114 - accuracy: 0.4877 - val_loss: 1.7125 - val_accuracy: 0.3545\n",
      "Epoch 73/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 1.4010 - accuracy: 0.4669 - val_loss: 1.7198 - val_accuracy: 0.3598\n",
      "Epoch 74/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 1.3962 - accuracy: 0.4747 - val_loss: 1.6674 - val_accuracy: 0.3968\n",
      "Epoch 75/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 1.3935 - accuracy: 0.4760 - val_loss: 1.6495 - val_accuracy: 0.3915\n",
      "Epoch 76/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 1.3847 - accuracy: 0.4890 - val_loss: 1.7604 - val_accuracy: 0.3016\n",
      "Epoch 77/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 1.3854 - accuracy: 0.4786 - val_loss: 1.6684 - val_accuracy: 0.3810\n",
      "Epoch 78/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 1.3867 - accuracy: 0.4812 - val_loss: 1.7206 - val_accuracy: 0.3333\n",
      "Epoch 79/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 1.3784 - accuracy: 0.4773 - val_loss: 1.7207 - val_accuracy: 0.3757\n",
      "Epoch 80/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 1.3801 - accuracy: 0.4864 - val_loss: 1.7024 - val_accuracy: 0.3757\n",
      "Epoch 81/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 1.3760 - accuracy: 0.4630 - val_loss: 1.6734 - val_accuracy: 0.3704\n",
      "Epoch 82/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 1.3629 - accuracy: 0.4825 - val_loss: 1.6741 - val_accuracy: 0.3651\n",
      "Epoch 83/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 1.3614 - accuracy: 0.4903 - val_loss: 1.6223 - val_accuracy: 0.4127\n",
      "Epoch 84/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 1.3579 - accuracy: 0.4981 - val_loss: 1.7294 - val_accuracy: 0.3704\n",
      "Epoch 85/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 1.3413 - accuracy: 0.4851 - val_loss: 1.6978 - val_accuracy: 0.3651\n",
      "Epoch 86/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 1.3494 - accuracy: 0.4968 - val_loss: 1.6876 - val_accuracy: 0.3598\n",
      "Epoch 87/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 1.3480 - accuracy: 0.4799 - val_loss: 1.6368 - val_accuracy: 0.3968\n",
      "Epoch 88/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 1.3401 - accuracy: 0.5019 - val_loss: 1.6679 - val_accuracy: 0.3704\n",
      "Epoch 89/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 1.3336 - accuracy: 0.5110 - val_loss: 1.6211 - val_accuracy: 0.3757\n",
      "Epoch 90/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 1.3284 - accuracy: 0.4994 - val_loss: 1.6459 - val_accuracy: 0.3915\n",
      "Epoch 91/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 1.3380 - accuracy: 0.4981 - val_loss: 1.6081 - val_accuracy: 0.3651\n",
      "Epoch 92/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 1.3213 - accuracy: 0.5214 - val_loss: 1.6115 - val_accuracy: 0.4127\n",
      "Epoch 93/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 1.3168 - accuracy: 0.5227 - val_loss: 1.6496 - val_accuracy: 0.3810\n",
      "Epoch 94/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 1.3190 - accuracy: 0.5045 - val_loss: 1.6091 - val_accuracy: 0.3862\n",
      "Epoch 95/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 1.3120 - accuracy: 0.5058 - val_loss: 1.6089 - val_accuracy: 0.3968\n",
      "Epoch 96/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 1.3056 - accuracy: 0.5136 - val_loss: 1.7377 - val_accuracy: 0.3386\n",
      "Epoch 97/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 1.3196 - accuracy: 0.4968 - val_loss: 1.6243 - val_accuracy: 0.4180\n",
      "Epoch 98/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 1.3011 - accuracy: 0.5188 - val_loss: 1.6226 - val_accuracy: 0.3915\n",
      "Epoch 99/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 1.3039 - accuracy: 0.5019 - val_loss: 1.6157 - val_accuracy: 0.3968\n",
      "Epoch 100/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 1.3019 - accuracy: 0.5136 - val_loss: 1.5854 - val_accuracy: 0.4180\n",
      "Epoch 101/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 1.2954 - accuracy: 0.5175 - val_loss: 1.6273 - val_accuracy: 0.3968\n",
      "Epoch 102/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 1.2968 - accuracy: 0.5071 - val_loss: 1.6093 - val_accuracy: 0.3915\n",
      "Epoch 103/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 1.2854 - accuracy: 0.5136 - val_loss: 1.6556 - val_accuracy: 0.3704\n",
      "Epoch 104/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 1.2836 - accuracy: 0.5162 - val_loss: 1.6478 - val_accuracy: 0.3862\n",
      "Epoch 105/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 1.2843 - accuracy: 0.5188 - val_loss: 1.6228 - val_accuracy: 0.3598\n",
      "Epoch 106/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 1.2776 - accuracy: 0.5266 - val_loss: 1.6720 - val_accuracy: 0.3651\n",
      "Epoch 107/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 1.2775 - accuracy: 0.5305 - val_loss: 1.6230 - val_accuracy: 0.4021\n",
      "Epoch 108/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 1.2733 - accuracy: 0.5409 - val_loss: 1.5907 - val_accuracy: 0.4074\n",
      "Epoch 109/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 1.2697 - accuracy: 0.5266 - val_loss: 1.6148 - val_accuracy: 0.3862\n",
      "Epoch 110/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 1.2607 - accuracy: 0.5305 - val_loss: 1.6246 - val_accuracy: 0.3704\n",
      "Epoch 111/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 1.2571 - accuracy: 0.5266 - val_loss: 1.6541 - val_accuracy: 0.3915\n",
      "Epoch 112/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 1.2628 - accuracy: 0.5149 - val_loss: 1.6058 - val_accuracy: 0.3862\n",
      "Epoch 113/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 1.2552 - accuracy: 0.5266 - val_loss: 1.6822 - val_accuracy: 0.3757\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 114/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 1.2566 - accuracy: 0.5331 - val_loss: 1.5780 - val_accuracy: 0.4127\n",
      "Epoch 115/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 1.2477 - accuracy: 0.5331 - val_loss: 1.6750 - val_accuracy: 0.4127\n",
      "Epoch 116/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 1.2522 - accuracy: 0.5188 - val_loss: 1.5893 - val_accuracy: 0.4127\n",
      "Epoch 117/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 1.2394 - accuracy: 0.5279 - val_loss: 1.5922 - val_accuracy: 0.4074\n",
      "Epoch 118/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 1.2439 - accuracy: 0.5473 - val_loss: 1.5907 - val_accuracy: 0.3862\n",
      "Epoch 119/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 1.2471 - accuracy: 0.5422 - val_loss: 1.5772 - val_accuracy: 0.4233\n",
      "Epoch 120/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 1.2355 - accuracy: 0.5422 - val_loss: 1.5731 - val_accuracy: 0.4127\n",
      "Epoch 121/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 1.2317 - accuracy: 0.5396 - val_loss: 1.5973 - val_accuracy: 0.3968\n",
      "Epoch 122/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 1.2372 - accuracy: 0.5499 - val_loss: 1.6523 - val_accuracy: 0.3545\n",
      "Epoch 123/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 1.2249 - accuracy: 0.5447 - val_loss: 1.7134 - val_accuracy: 0.3757\n",
      "Epoch 124/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 1.2333 - accuracy: 0.5318 - val_loss: 1.6041 - val_accuracy: 0.4233\n",
      "Epoch 125/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 1.2254 - accuracy: 0.5227 - val_loss: 1.5769 - val_accuracy: 0.4074\n",
      "Epoch 126/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 1.2287 - accuracy: 0.5396 - val_loss: 1.6467 - val_accuracy: 0.3968\n",
      "Epoch 127/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 1.2172 - accuracy: 0.5447 - val_loss: 1.5669 - val_accuracy: 0.4286\n",
      "Epoch 128/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 1.2163 - accuracy: 0.5370 - val_loss: 1.6225 - val_accuracy: 0.3915\n",
      "Epoch 129/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 1.2204 - accuracy: 0.5512 - val_loss: 1.6439 - val_accuracy: 0.3862\n",
      "Epoch 130/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 1.2124 - accuracy: 0.5525 - val_loss: 1.6044 - val_accuracy: 0.4127\n",
      "Epoch 131/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 1.2166 - accuracy: 0.5577 - val_loss: 1.5758 - val_accuracy: 0.4233\n",
      "Epoch 132/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 1.2049 - accuracy: 0.5383 - val_loss: 1.6023 - val_accuracy: 0.4127\n",
      "Epoch 133/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 1.2078 - accuracy: 0.5383 - val_loss: 1.6255 - val_accuracy: 0.3545\n",
      "Epoch 134/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 1.2059 - accuracy: 0.5344 - val_loss: 1.6166 - val_accuracy: 0.3915\n",
      "Epoch 135/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 1.1966 - accuracy: 0.5577 - val_loss: 1.5396 - val_accuracy: 0.4180\n",
      "Epoch 136/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 1.2011 - accuracy: 0.5655 - val_loss: 1.6357 - val_accuracy: 0.3598\n",
      "Epoch 137/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 1.1983 - accuracy: 0.5538 - val_loss: 1.5697 - val_accuracy: 0.4444\n",
      "Epoch 138/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 1.1952 - accuracy: 0.5577 - val_loss: 1.6052 - val_accuracy: 0.3968\n",
      "Epoch 139/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 1.1932 - accuracy: 0.5422 - val_loss: 1.5896 - val_accuracy: 0.4180\n",
      "Epoch 140/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 1.1915 - accuracy: 0.5486 - val_loss: 1.5839 - val_accuracy: 0.4074\n",
      "Epoch 141/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 1.1846 - accuracy: 0.5525 - val_loss: 1.6320 - val_accuracy: 0.4180\n",
      "Epoch 142/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 1.1892 - accuracy: 0.5525 - val_loss: 1.7197 - val_accuracy: 0.3598\n",
      "Epoch 143/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 1.1760 - accuracy: 0.5668 - val_loss: 1.5266 - val_accuracy: 0.4339\n",
      "Epoch 144/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 1.1854 - accuracy: 0.5707 - val_loss: 1.5989 - val_accuracy: 0.3704\n",
      "Epoch 145/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 1.1746 - accuracy: 0.5512 - val_loss: 1.5477 - val_accuracy: 0.4127\n",
      "Epoch 146/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 1.1748 - accuracy: 0.5655 - val_loss: 1.5629 - val_accuracy: 0.3915\n",
      "Epoch 147/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 1.1698 - accuracy: 0.5512 - val_loss: 1.6400 - val_accuracy: 0.3862\n",
      "Epoch 148/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 1.1772 - accuracy: 0.5746 - val_loss: 1.6247 - val_accuracy: 0.3968\n",
      "Epoch 149/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 1.1742 - accuracy: 0.5603 - val_loss: 1.5411 - val_accuracy: 0.4392\n",
      "Epoch 150/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 1.1696 - accuracy: 0.5694 - val_loss: 1.6194 - val_accuracy: 0.3862\n",
      "Epoch 151/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 1.1551 - accuracy: 0.5616 - val_loss: 1.7528 - val_accuracy: 0.3598\n",
      "Epoch 152/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 1.1687 - accuracy: 0.5798 - val_loss: 1.5704 - val_accuracy: 0.4127\n",
      "Epoch 153/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 1.1607 - accuracy: 0.5772 - val_loss: 1.5542 - val_accuracy: 0.4444\n",
      "Epoch 154/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 1.1534 - accuracy: 0.5733 - val_loss: 1.6491 - val_accuracy: 0.4550\n",
      "Epoch 155/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 1.1552 - accuracy: 0.5901 - val_loss: 1.5613 - val_accuracy: 0.4233\n",
      "Epoch 156/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 1.1551 - accuracy: 0.5733 - val_loss: 1.5629 - val_accuracy: 0.4074\n",
      "Epoch 157/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 1.1507 - accuracy: 0.5759 - val_loss: 1.6817 - val_accuracy: 0.3915\n",
      "Epoch 158/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 1.1556 - accuracy: 0.5720 - val_loss: 1.6021 - val_accuracy: 0.4074\n",
      "Epoch 159/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 1.1364 - accuracy: 0.5837 - val_loss: 1.5468 - val_accuracy: 0.4286\n",
      "Epoch 160/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 1.1501 - accuracy: 0.5668 - val_loss: 1.5878 - val_accuracy: 0.4392\n",
      "Epoch 161/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 1.1426 - accuracy: 0.5811 - val_loss: 1.6131 - val_accuracy: 0.4339\n",
      "Epoch 162/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 1.1450 - accuracy: 0.5888 - val_loss: 1.5898 - val_accuracy: 0.4286\n",
      "Epoch 163/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 1.1460 - accuracy: 0.5707 - val_loss: 1.5758 - val_accuracy: 0.4074\n",
      "Epoch 164/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 1.1340 - accuracy: 0.5798 - val_loss: 1.5861 - val_accuracy: 0.4233\n",
      "Epoch 165/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 1.1326 - accuracy: 0.5888 - val_loss: 1.5829 - val_accuracy: 0.3915\n",
      "Epoch 166/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 1.1340 - accuracy: 0.5746 - val_loss: 1.6363 - val_accuracy: 0.3862\n",
      "Epoch 167/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 1.1229 - accuracy: 0.5694 - val_loss: 1.6718 - val_accuracy: 0.3386\n",
      "Epoch 168/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 1.1372 - accuracy: 0.5875 - val_loss: 1.5899 - val_accuracy: 0.4180\n",
      "Epoch 169/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 1.1226 - accuracy: 0.5979 - val_loss: 1.5635 - val_accuracy: 0.3915\n",
      "Epoch 170/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 1.1263 - accuracy: 0.5746 - val_loss: 1.5727 - val_accuracy: 0.3968\n",
      "Epoch 171/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 1.1137 - accuracy: 0.5888 - val_loss: 1.5717 - val_accuracy: 0.3968\n",
      "Epoch 172/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 1.1177 - accuracy: 0.5850 - val_loss: 1.5510 - val_accuracy: 0.4444\n",
      "Epoch 173/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 1.1131 - accuracy: 0.5914 - val_loss: 1.5847 - val_accuracy: 0.4286\n",
      "Epoch 174/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 1.1088 - accuracy: 0.6005 - val_loss: 1.6200 - val_accuracy: 0.4021\n",
      "Epoch 175/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 1.1141 - accuracy: 0.5888 - val_loss: 1.5601 - val_accuracy: 0.4286\n",
      "Epoch 176/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 1.0979 - accuracy: 0.6083 - val_loss: 1.6705 - val_accuracy: 0.3968\n",
      "Epoch 177/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 1.1194 - accuracy: 0.5914 - val_loss: 1.5524 - val_accuracy: 0.4233\n",
      "Epoch 178/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 1.1054 - accuracy: 0.5875 - val_loss: 1.5958 - val_accuracy: 0.4127\n",
      "Epoch 179/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 1.1100 - accuracy: 0.6057 - val_loss: 1.5753 - val_accuracy: 0.4180\n",
      "Epoch 180/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 1.1030 - accuracy: 0.5927 - val_loss: 1.6157 - val_accuracy: 0.3968\n",
      "Epoch 181/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 1.1038 - accuracy: 0.6018 - val_loss: 1.5411 - val_accuracy: 0.4497\n",
      "Epoch 182/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 1.0931 - accuracy: 0.5927 - val_loss: 1.5742 - val_accuracy: 0.4286\n",
      "Epoch 183/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 1.0890 - accuracy: 0.6083 - val_loss: 1.5614 - val_accuracy: 0.4339\n",
      "Epoch 184/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 1.0959 - accuracy: 0.6057 - val_loss: 1.7313 - val_accuracy: 0.3757\n",
      "Epoch 185/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 1.1001 - accuracy: 0.6018 - val_loss: 1.5499 - val_accuracy: 0.4074\n",
      "Epoch 186/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 1.0895 - accuracy: 0.5927 - val_loss: 1.6292 - val_accuracy: 0.3915\n",
      "Epoch 187/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 1.0757 - accuracy: 0.6239 - val_loss: 1.6022 - val_accuracy: 0.4286\n",
      "Epoch 188/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 1.0837 - accuracy: 0.6044 - val_loss: 1.6054 - val_accuracy: 0.4286\n",
      "Epoch 189/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 1.0785 - accuracy: 0.6109 - val_loss: 1.5574 - val_accuracy: 0.4127\n",
      "Epoch 190/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 1.0840 - accuracy: 0.6005 - val_loss: 1.5804 - val_accuracy: 0.4021\n",
      "Epoch 191/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 1.0858 - accuracy: 0.6148 - val_loss: 1.5947 - val_accuracy: 0.4074\n",
      "Epoch 192/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 1.0790 - accuracy: 0.6057 - val_loss: 1.5954 - val_accuracy: 0.4021\n",
      "Epoch 193/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 1.0733 - accuracy: 0.6122 - val_loss: 1.5975 - val_accuracy: 0.4339\n",
      "Epoch 194/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 1.0723 - accuracy: 0.6187 - val_loss: 1.5167 - val_accuracy: 0.4233\n",
      "Epoch 195/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 1.0716 - accuracy: 0.6083 - val_loss: 1.5584 - val_accuracy: 0.4180\n",
      "Epoch 196/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 1.0685 - accuracy: 0.6044 - val_loss: 1.5673 - val_accuracy: 0.4392\n",
      "Epoch 197/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 1.0688 - accuracy: 0.6265 - val_loss: 1.5380 - val_accuracy: 0.4180\n",
      "Epoch 198/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 1.0711 - accuracy: 0.5927 - val_loss: 1.6199 - val_accuracy: 0.3915\n",
      "Epoch 199/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 1.0612 - accuracy: 0.6174 - val_loss: 1.5940 - val_accuracy: 0.4392\n",
      "Epoch 200/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 1.0582 - accuracy: 0.6057 - val_loss: 1.6269 - val_accuracy: 0.4074\n",
      "Epoch 201/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 1.0685 - accuracy: 0.6083 - val_loss: 1.6020 - val_accuracy: 0.4021\n",
      "Epoch 202/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 1.0728 - accuracy: 0.6057 - val_loss: 1.5528 - val_accuracy: 0.4339\n",
      "Epoch 203/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 1.0525 - accuracy: 0.6304 - val_loss: 1.5311 - val_accuracy: 0.4392\n",
      "Epoch 204/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 1.0567 - accuracy: 0.6096 - val_loss: 1.6135 - val_accuracy: 0.4233\n",
      "Epoch 205/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 1.0493 - accuracy: 0.6265 - val_loss: 1.5178 - val_accuracy: 0.4392\n",
      "Epoch 206/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 1.0537 - accuracy: 0.6278 - val_loss: 1.5387 - val_accuracy: 0.4339\n",
      "Epoch 207/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 1.0506 - accuracy: 0.6239 - val_loss: 1.6980 - val_accuracy: 0.3810\n",
      "Epoch 208/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 1.0501 - accuracy: 0.6135 - val_loss: 1.6177 - val_accuracy: 0.3704\n",
      "Epoch 209/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 1.0443 - accuracy: 0.6122 - val_loss: 1.7557 - val_accuracy: 0.3439\n",
      "Epoch 210/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 1.0563 - accuracy: 0.6031 - val_loss: 1.5714 - val_accuracy: 0.3810\n",
      "Epoch 211/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 1.0421 - accuracy: 0.6381 - val_loss: 1.6191 - val_accuracy: 0.4127\n",
      "Epoch 212/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 1.0445 - accuracy: 0.6252 - val_loss: 1.6090 - val_accuracy: 0.4127\n",
      "Epoch 213/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 1.0342 - accuracy: 0.6329 - val_loss: 1.6240 - val_accuracy: 0.4392\n",
      "Epoch 214/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 1.0460 - accuracy: 0.6187 - val_loss: 1.5373 - val_accuracy: 0.4392\n",
      "Epoch 215/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 1.0381 - accuracy: 0.6407 - val_loss: 1.6412 - val_accuracy: 0.4233\n",
      "Epoch 216/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 1.0294 - accuracy: 0.6265 - val_loss: 1.6045 - val_accuracy: 0.4021\n",
      "Epoch 217/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 1.0394 - accuracy: 0.6291 - val_loss: 1.6152 - val_accuracy: 0.4392\n",
      "Epoch 218/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 1.0282 - accuracy: 0.6213 - val_loss: 1.6522 - val_accuracy: 0.3810\n",
      "Epoch 219/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 1.0305 - accuracy: 0.6148 - val_loss: 1.5829 - val_accuracy: 0.4286\n",
      "Epoch 220/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 1.0233 - accuracy: 0.6316 - val_loss: 1.5963 - val_accuracy: 0.4180\n",
      "Epoch 221/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 1.0260 - accuracy: 0.6446 - val_loss: 1.5606 - val_accuracy: 0.4497\n",
      "Epoch 222/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 1.0266 - accuracy: 0.6316 - val_loss: 1.5213 - val_accuracy: 0.4550\n",
      "Epoch 223/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 1.0183 - accuracy: 0.6420 - val_loss: 1.5388 - val_accuracy: 0.4603\n",
      "Epoch 224/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 1.0192 - accuracy: 0.6342 - val_loss: 1.5550 - val_accuracy: 0.4180\n",
      "Epoch 225/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 1.0136 - accuracy: 0.6368 - val_loss: 1.5524 - val_accuracy: 0.4392\n",
      "Epoch 226/700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "771/771 [==============================] - 4s 5ms/step - loss: 1.0188 - accuracy: 0.6394 - val_loss: 1.5667 - val_accuracy: 0.4550\n",
      "Epoch 227/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 1.0159 - accuracy: 0.6329 - val_loss: 1.5762 - val_accuracy: 0.4286\n",
      "Epoch 228/700\n",
      "771/771 [==============================] - 3s 4ms/step - loss: 1.0042 - accuracy: 0.6407 - val_loss: 1.5763 - val_accuracy: 0.4339\n",
      "Epoch 229/700\n",
      "771/771 [==============================] - 3s 4ms/step - loss: 1.0068 - accuracy: 0.6355 - val_loss: 1.5841 - val_accuracy: 0.4286\n",
      "Epoch 230/700\n",
      "771/771 [==============================] - 3s 4ms/step - loss: 1.0013 - accuracy: 0.6252 - val_loss: 1.5555 - val_accuracy: 0.4497\n",
      "Epoch 231/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 1.0105 - accuracy: 0.6433 - val_loss: 1.5896 - val_accuracy: 0.4286\n",
      "Epoch 232/700\n",
      "771/771 [==============================] - 3s 4ms/step - loss: 1.0027 - accuracy: 0.6342 - val_loss: 1.6475 - val_accuracy: 0.4339\n",
      "Epoch 233/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 1.0013 - accuracy: 0.6329 - val_loss: 1.5935 - val_accuracy: 0.4180\n",
      "Epoch 234/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.9938 - accuracy: 0.6589 - val_loss: 1.6193 - val_accuracy: 0.4339\n",
      "Epoch 235/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 1.0006 - accuracy: 0.6537 - val_loss: 1.5858 - val_accuracy: 0.3810\n",
      "Epoch 236/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.9960 - accuracy: 0.6667 - val_loss: 1.6629 - val_accuracy: 0.3915\n",
      "Epoch 237/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 1.0013 - accuracy: 0.6355 - val_loss: 1.6240 - val_accuracy: 0.3915\n",
      "Epoch 238/700\n",
      "771/771 [==============================] - 3s 4ms/step - loss: 0.9952 - accuracy: 0.6394 - val_loss: 1.5154 - val_accuracy: 0.4233\n",
      "Epoch 239/700\n",
      "771/771 [==============================] - 3s 4ms/step - loss: 0.9936 - accuracy: 0.6433 - val_loss: 1.5642 - val_accuracy: 0.4444\n",
      "Epoch 240/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.9873 - accuracy: 0.6524 - val_loss: 1.6373 - val_accuracy: 0.4127\n",
      "Epoch 241/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.9765 - accuracy: 0.6563 - val_loss: 1.6773 - val_accuracy: 0.3757\n",
      "Epoch 242/700\n",
      "771/771 [==============================] - 3s 4ms/step - loss: 0.9806 - accuracy: 0.6680 - val_loss: 1.5782 - val_accuracy: 0.4074\n",
      "Epoch 243/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.9817 - accuracy: 0.6407 - val_loss: 1.6623 - val_accuracy: 0.3757\n",
      "Epoch 244/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.9819 - accuracy: 0.6563 - val_loss: 1.5973 - val_accuracy: 0.3968\n",
      "Epoch 245/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.9806 - accuracy: 0.6589 - val_loss: 1.6680 - val_accuracy: 0.3915\n",
      "Epoch 246/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.9820 - accuracy: 0.6498 - val_loss: 1.5940 - val_accuracy: 0.4233\n",
      "Epoch 247/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.9698 - accuracy: 0.6265 - val_loss: 1.5831 - val_accuracy: 0.4233\n",
      "Epoch 248/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.9771 - accuracy: 0.6628 - val_loss: 1.6228 - val_accuracy: 0.4180\n",
      "Epoch 249/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.9747 - accuracy: 0.6485 - val_loss: 1.6405 - val_accuracy: 0.3598\n",
      "Epoch 250/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.9729 - accuracy: 0.6563 - val_loss: 1.6021 - val_accuracy: 0.4392\n",
      "Epoch 251/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.9679 - accuracy: 0.6654 - val_loss: 1.5765 - val_accuracy: 0.4021\n",
      "Epoch 252/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.9649 - accuracy: 0.6628 - val_loss: 1.5404 - val_accuracy: 0.4392\n",
      "Epoch 253/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.9661 - accuracy: 0.6550 - val_loss: 1.5623 - val_accuracy: 0.4180\n",
      "Epoch 254/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.9659 - accuracy: 0.6628 - val_loss: 1.5644 - val_accuracy: 0.4180\n",
      "Epoch 255/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.9565 - accuracy: 0.6472 - val_loss: 1.6213 - val_accuracy: 0.4233\n",
      "Epoch 256/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.9642 - accuracy: 0.6719 - val_loss: 1.7384 - val_accuracy: 0.3757\n",
      "Epoch 257/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.9560 - accuracy: 0.6498 - val_loss: 1.6340 - val_accuracy: 0.4074\n",
      "Epoch 258/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.9560 - accuracy: 0.6667 - val_loss: 1.6632 - val_accuracy: 0.4180\n",
      "Epoch 259/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.9603 - accuracy: 0.6654 - val_loss: 1.5638 - val_accuracy: 0.4444\n",
      "Epoch 260/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.9540 - accuracy: 0.6719 - val_loss: 1.5607 - val_accuracy: 0.4074\n",
      "Epoch 261/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.9546 - accuracy: 0.6550 - val_loss: 1.6008 - val_accuracy: 0.4339\n",
      "Epoch 262/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.9546 - accuracy: 0.6809 - val_loss: 1.5692 - val_accuracy: 0.4339\n",
      "Epoch 263/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.9518 - accuracy: 0.6706 - val_loss: 1.5955 - val_accuracy: 0.4233\n",
      "Epoch 264/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.9456 - accuracy: 0.6641 - val_loss: 1.6490 - val_accuracy: 0.4180\n",
      "Epoch 265/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.9428 - accuracy: 0.6615 - val_loss: 1.5432 - val_accuracy: 0.4074\n",
      "Epoch 266/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.9367 - accuracy: 0.6783 - val_loss: 1.7631 - val_accuracy: 0.3915\n",
      "Epoch 267/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.9433 - accuracy: 0.6641 - val_loss: 1.5636 - val_accuracy: 0.4286\n",
      "Epoch 268/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.9372 - accuracy: 0.6615 - val_loss: 1.5470 - val_accuracy: 0.4180\n",
      "Epoch 269/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.9471 - accuracy: 0.6744 - val_loss: 1.5871 - val_accuracy: 0.4286\n",
      "Epoch 270/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.9404 - accuracy: 0.6615 - val_loss: 1.5892 - val_accuracy: 0.4074\n",
      "Epoch 271/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.9334 - accuracy: 0.6822 - val_loss: 1.5501 - val_accuracy: 0.4286\n",
      "Epoch 272/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.9304 - accuracy: 0.6589 - val_loss: 1.5993 - val_accuracy: 0.4127\n",
      "Epoch 273/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.9263 - accuracy: 0.6796 - val_loss: 1.5581 - val_accuracy: 0.4339\n",
      "Epoch 274/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.9385 - accuracy: 0.6822 - val_loss: 1.5894 - val_accuracy: 0.4233\n",
      "Epoch 275/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.9260 - accuracy: 0.6796 - val_loss: 1.6548 - val_accuracy: 0.4074\n",
      "Epoch 276/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.9272 - accuracy: 0.6809 - val_loss: 1.5876 - val_accuracy: 0.4286\n",
      "Epoch 277/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.9222 - accuracy: 0.6602 - val_loss: 1.5687 - val_accuracy: 0.4392\n",
      "Epoch 278/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.9165 - accuracy: 0.6822 - val_loss: 1.6509 - val_accuracy: 0.4074\n",
      "Epoch 279/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.9185 - accuracy: 0.6796 - val_loss: 1.6352 - val_accuracy: 0.3915\n",
      "Epoch 280/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.9230 - accuracy: 0.6757 - val_loss: 1.6840 - val_accuracy: 0.3704\n",
      "Epoch 281/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.9178 - accuracy: 0.6861 - val_loss: 1.5629 - val_accuracy: 0.4233\n",
      "Epoch 282/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.9189 - accuracy: 0.6706 - val_loss: 1.6576 - val_accuracy: 0.4021\n",
      "Epoch 283/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.9159 - accuracy: 0.6770 - val_loss: 1.5811 - val_accuracy: 0.4392\n",
      "Epoch 284/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.9165 - accuracy: 0.6706 - val_loss: 1.6736 - val_accuracy: 0.3968\n",
      "Epoch 285/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.9068 - accuracy: 0.6952 - val_loss: 1.5878 - val_accuracy: 0.4233\n",
      "Epoch 286/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.9058 - accuracy: 0.6900 - val_loss: 1.6462 - val_accuracy: 0.4233\n",
      "Epoch 287/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.9061 - accuracy: 0.6887 - val_loss: 1.5495 - val_accuracy: 0.4392\n",
      "Epoch 288/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.9008 - accuracy: 0.6809 - val_loss: 1.6920 - val_accuracy: 0.3915\n",
      "Epoch 289/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.8992 - accuracy: 0.6913 - val_loss: 1.5959 - val_accuracy: 0.4180\n",
      "Epoch 290/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.8941 - accuracy: 0.6783 - val_loss: 1.6552 - val_accuracy: 0.4497\n",
      "Epoch 291/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.9026 - accuracy: 0.6978 - val_loss: 1.6731 - val_accuracy: 0.3915\n",
      "Epoch 292/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.9001 - accuracy: 0.6991 - val_loss: 1.5943 - val_accuracy: 0.4180\n",
      "Epoch 293/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.8875 - accuracy: 0.6874 - val_loss: 1.5856 - val_accuracy: 0.4603\n",
      "Epoch 294/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.8975 - accuracy: 0.6861 - val_loss: 1.5909 - val_accuracy: 0.4074\n",
      "Epoch 295/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.8987 - accuracy: 0.6835 - val_loss: 1.5717 - val_accuracy: 0.4286\n",
      "Epoch 296/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.8885 - accuracy: 0.7004 - val_loss: 1.6058 - val_accuracy: 0.4021\n",
      "Epoch 297/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.9000 - accuracy: 0.6952 - val_loss: 1.5761 - val_accuracy: 0.4127\n",
      "Epoch 298/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.8844 - accuracy: 0.6952 - val_loss: 1.6102 - val_accuracy: 0.4286\n",
      "Epoch 299/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.8823 - accuracy: 0.6835 - val_loss: 1.6017 - val_accuracy: 0.3915\n",
      "Epoch 300/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.8752 - accuracy: 0.7004 - val_loss: 1.6563 - val_accuracy: 0.4074\n",
      "Epoch 301/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.8850 - accuracy: 0.6861 - val_loss: 1.5329 - val_accuracy: 0.4074\n",
      "Epoch 302/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.8786 - accuracy: 0.6965 - val_loss: 1.6084 - val_accuracy: 0.4233\n",
      "Epoch 303/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.8833 - accuracy: 0.7030 - val_loss: 1.6198 - val_accuracy: 0.4392\n",
      "Epoch 304/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.8776 - accuracy: 0.6835 - val_loss: 1.6266 - val_accuracy: 0.4127\n",
      "Epoch 305/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.8750 - accuracy: 0.6939 - val_loss: 1.5309 - val_accuracy: 0.4392\n",
      "Epoch 306/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.8653 - accuracy: 0.6874 - val_loss: 1.6192 - val_accuracy: 0.4392\n",
      "Epoch 307/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.8769 - accuracy: 0.6822 - val_loss: 1.7423 - val_accuracy: 0.3704\n",
      "Epoch 308/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.8732 - accuracy: 0.7069 - val_loss: 1.5791 - val_accuracy: 0.4233\n",
      "Epoch 309/700\n",
      "771/771 [==============================] - 3s 5ms/step - loss: 0.8762 - accuracy: 0.6874 - val_loss: 1.6020 - val_accuracy: 0.4180\n",
      "Epoch 310/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.8675 - accuracy: 0.7056 - val_loss: 1.5279 - val_accuracy: 0.4339\n",
      "Epoch 311/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.8706 - accuracy: 0.7043 - val_loss: 1.5810 - val_accuracy: 0.4550\n",
      "Epoch 312/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.8672 - accuracy: 0.6952 - val_loss: 1.5627 - val_accuracy: 0.4233\n",
      "Epoch 313/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.8616 - accuracy: 0.7004 - val_loss: 1.7300 - val_accuracy: 0.4180\n",
      "Epoch 314/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.8601 - accuracy: 0.7017 - val_loss: 1.6392 - val_accuracy: 0.4074\n",
      "Epoch 315/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.8590 - accuracy: 0.7030 - val_loss: 1.6823 - val_accuracy: 0.4021\n",
      "Epoch 316/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.8574 - accuracy: 0.7160 - val_loss: 1.5849 - val_accuracy: 0.4603\n",
      "Epoch 317/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.8549 - accuracy: 0.7082 - val_loss: 1.6131 - val_accuracy: 0.4233\n",
      "Epoch 318/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.8531 - accuracy: 0.7121 - val_loss: 1.6247 - val_accuracy: 0.4127\n",
      "Epoch 319/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.8559 - accuracy: 0.7043 - val_loss: 1.5504 - val_accuracy: 0.4127\n",
      "Epoch 320/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.8547 - accuracy: 0.7069 - val_loss: 1.5834 - val_accuracy: 0.4444\n",
      "Epoch 321/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.8494 - accuracy: 0.7173 - val_loss: 1.5694 - val_accuracy: 0.4180\n",
      "Epoch 322/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.8466 - accuracy: 0.7185 - val_loss: 1.6333 - val_accuracy: 0.4127\n",
      "Epoch 323/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.8470 - accuracy: 0.7147 - val_loss: 1.5462 - val_accuracy: 0.4286\n",
      "Epoch 324/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.8451 - accuracy: 0.7069 - val_loss: 1.6581 - val_accuracy: 0.3862\n",
      "Epoch 325/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.8432 - accuracy: 0.7185 - val_loss: 1.6065 - val_accuracy: 0.4339\n",
      "Epoch 326/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.8333 - accuracy: 0.7160 - val_loss: 1.6043 - val_accuracy: 0.4180\n",
      "Epoch 327/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.8352 - accuracy: 0.7198 - val_loss: 1.6287 - val_accuracy: 0.4233\n",
      "Epoch 328/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.8316 - accuracy: 0.7134 - val_loss: 1.5680 - val_accuracy: 0.4550\n",
      "Epoch 329/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.8377 - accuracy: 0.7121 - val_loss: 1.5578 - val_accuracy: 0.4286\n",
      "Epoch 330/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.8399 - accuracy: 0.7095 - val_loss: 1.5857 - val_accuracy: 0.4444\n",
      "Epoch 331/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.8346 - accuracy: 0.7211 - val_loss: 1.5984 - val_accuracy: 0.4021\n",
      "Epoch 332/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.8246 - accuracy: 0.7185 - val_loss: 1.6732 - val_accuracy: 0.4339\n",
      "Epoch 333/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.8339 - accuracy: 0.7108 - val_loss: 1.6112 - val_accuracy: 0.4550\n",
      "Epoch 334/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.8201 - accuracy: 0.7250 - val_loss: 1.6880 - val_accuracy: 0.4339\n",
      "Epoch 335/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.8244 - accuracy: 0.7185 - val_loss: 1.6089 - val_accuracy: 0.4021\n",
      "Epoch 336/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.8289 - accuracy: 0.7095 - val_loss: 1.6000 - val_accuracy: 0.4233\n",
      "Epoch 337/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.8172 - accuracy: 0.7328 - val_loss: 1.6397 - val_accuracy: 0.4074\n",
      "Epoch 338/700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "771/771 [==============================] - 4s 5ms/step - loss: 0.8143 - accuracy: 0.7289 - val_loss: 1.5758 - val_accuracy: 0.4180\n",
      "Epoch 339/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.8188 - accuracy: 0.7082 - val_loss: 1.5312 - val_accuracy: 0.4233\n",
      "Epoch 340/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.8186 - accuracy: 0.7393 - val_loss: 1.7089 - val_accuracy: 0.3810\n",
      "Epoch 341/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.8130 - accuracy: 0.7263 - val_loss: 1.6232 - val_accuracy: 0.4021\n",
      "Epoch 342/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.8074 - accuracy: 0.7224 - val_loss: 1.6318 - val_accuracy: 0.4127\n",
      "Epoch 343/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.8167 - accuracy: 0.7211 - val_loss: 1.7342 - val_accuracy: 0.3651\n",
      "Epoch 344/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.8152 - accuracy: 0.7354 - val_loss: 1.6064 - val_accuracy: 0.4233\n",
      "Epoch 345/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.8075 - accuracy: 0.7250 - val_loss: 1.5827 - val_accuracy: 0.4127\n",
      "Epoch 346/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.8071 - accuracy: 0.7289 - val_loss: 1.5888 - val_accuracy: 0.4392\n",
      "Epoch 347/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.8058 - accuracy: 0.7315 - val_loss: 1.6100 - val_accuracy: 0.4339\n",
      "Epoch 348/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.8100 - accuracy: 0.7289 - val_loss: 1.6252 - val_accuracy: 0.4180\n",
      "Epoch 349/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.8031 - accuracy: 0.7276 - val_loss: 1.5844 - val_accuracy: 0.4392\n",
      "Epoch 350/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.8018 - accuracy: 0.7173 - val_loss: 1.7346 - val_accuracy: 0.4233\n",
      "Epoch 351/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.7994 - accuracy: 0.7354 - val_loss: 1.6288 - val_accuracy: 0.4074\n",
      "Epoch 352/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.7895 - accuracy: 0.7484 - val_loss: 1.7292 - val_accuracy: 0.3968\n",
      "Epoch 353/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.7987 - accuracy: 0.7367 - val_loss: 1.6699 - val_accuracy: 0.4286\n",
      "Epoch 354/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.7944 - accuracy: 0.7289 - val_loss: 1.6224 - val_accuracy: 0.4074\n",
      "Epoch 355/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.7946 - accuracy: 0.7250 - val_loss: 1.6670 - val_accuracy: 0.4074\n",
      "Epoch 356/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.7888 - accuracy: 0.7289 - val_loss: 1.6099 - val_accuracy: 0.4233\n",
      "Epoch 357/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.7936 - accuracy: 0.7237 - val_loss: 1.6751 - val_accuracy: 0.4074\n",
      "Epoch 358/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.7831 - accuracy: 0.7289 - val_loss: 1.5760 - val_accuracy: 0.4127\n",
      "Epoch 359/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.7827 - accuracy: 0.7380 - val_loss: 1.6374 - val_accuracy: 0.4233\n",
      "Epoch 360/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.7864 - accuracy: 0.7276 - val_loss: 1.5847 - val_accuracy: 0.4339\n",
      "Epoch 361/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.7796 - accuracy: 0.7497 - val_loss: 1.6364 - val_accuracy: 0.4339\n",
      "Epoch 362/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.7817 - accuracy: 0.7263 - val_loss: 1.6365 - val_accuracy: 0.4074\n",
      "Epoch 363/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.7741 - accuracy: 0.7315 - val_loss: 1.6098 - val_accuracy: 0.4074\n",
      "Epoch 364/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.7725 - accuracy: 0.7393 - val_loss: 1.6304 - val_accuracy: 0.4286\n",
      "Epoch 365/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.7778 - accuracy: 0.7471 - val_loss: 1.7518 - val_accuracy: 0.4074\n",
      "Epoch 366/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.7751 - accuracy: 0.7393 - val_loss: 1.8079 - val_accuracy: 0.3651\n",
      "Epoch 367/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.7799 - accuracy: 0.7315 - val_loss: 1.5905 - val_accuracy: 0.4286\n",
      "Epoch 368/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.7749 - accuracy: 0.7380 - val_loss: 1.6065 - val_accuracy: 0.3968\n",
      "Epoch 369/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.7634 - accuracy: 0.7536 - val_loss: 1.6147 - val_accuracy: 0.3968\n",
      "Epoch 370/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.7719 - accuracy: 0.7328 - val_loss: 1.6231 - val_accuracy: 0.3968\n",
      "Epoch 371/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.7607 - accuracy: 0.7536 - val_loss: 1.6911 - val_accuracy: 0.4180\n",
      "Epoch 372/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.7662 - accuracy: 0.7484 - val_loss: 1.7083 - val_accuracy: 0.4180\n",
      "Epoch 373/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.7667 - accuracy: 0.7406 - val_loss: 1.6862 - val_accuracy: 0.3968\n",
      "Epoch 374/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.7567 - accuracy: 0.7315 - val_loss: 1.6096 - val_accuracy: 0.4021\n",
      "Epoch 375/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.7600 - accuracy: 0.7419 - val_loss: 1.6405 - val_accuracy: 0.4286\n",
      "Epoch 376/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.7539 - accuracy: 0.7432 - val_loss: 1.7199 - val_accuracy: 0.3915\n",
      "Epoch 377/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.7567 - accuracy: 0.7510 - val_loss: 1.6344 - val_accuracy: 0.4286\n",
      "Epoch 378/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.7513 - accuracy: 0.7484 - val_loss: 1.7220 - val_accuracy: 0.4233\n",
      "Epoch 379/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.7467 - accuracy: 0.7380 - val_loss: 1.6045 - val_accuracy: 0.4444\n",
      "Epoch 380/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.7439 - accuracy: 0.7575 - val_loss: 1.6824 - val_accuracy: 0.4339\n",
      "Epoch 381/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.7502 - accuracy: 0.7510 - val_loss: 1.6768 - val_accuracy: 0.4180\n",
      "Epoch 382/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.7448 - accuracy: 0.7536 - val_loss: 1.5957 - val_accuracy: 0.4286\n",
      "Epoch 383/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.7531 - accuracy: 0.7406 - val_loss: 1.6498 - val_accuracy: 0.4127\n",
      "Epoch 384/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.7396 - accuracy: 0.7523 - val_loss: 1.7801 - val_accuracy: 0.3862\n",
      "Epoch 385/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.7389 - accuracy: 0.7704 - val_loss: 1.6130 - val_accuracy: 0.4286\n",
      "Epoch 386/700\n",
      "771/771 [==============================] - 3s 4ms/step - loss: 0.7391 - accuracy: 0.7445 - val_loss: 1.6657 - val_accuracy: 0.4339\n",
      "Epoch 387/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.7367 - accuracy: 0.7458 - val_loss: 1.6869 - val_accuracy: 0.4127\n",
      "Epoch 388/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.7346 - accuracy: 0.7393 - val_loss: 1.6387 - val_accuracy: 0.4127\n",
      "Epoch 389/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.7301 - accuracy: 0.7652 - val_loss: 1.6476 - val_accuracy: 0.3968\n",
      "Epoch 390/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.7361 - accuracy: 0.7639 - val_loss: 1.6225 - val_accuracy: 0.4127\n",
      "Epoch 391/700\n",
      "771/771 [==============================] - 3s 4ms/step - loss: 0.7276 - accuracy: 0.7549 - val_loss: 1.7015 - val_accuracy: 0.4021\n",
      "Epoch 392/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.7469 - accuracy: 0.7588 - val_loss: 1.5979 - val_accuracy: 0.4339\n",
      "Epoch 393/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.7276 - accuracy: 0.7458 - val_loss: 1.6838 - val_accuracy: 0.4074\n",
      "Epoch 394/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.7355 - accuracy: 0.7665 - val_loss: 1.6245 - val_accuracy: 0.4233\n",
      "Epoch 395/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.7187 - accuracy: 0.7795 - val_loss: 1.5888 - val_accuracy: 0.4339\n",
      "Epoch 396/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.7180 - accuracy: 0.7613 - val_loss: 1.5783 - val_accuracy: 0.4021\n",
      "Epoch 397/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.7261 - accuracy: 0.7588 - val_loss: 1.7757 - val_accuracy: 0.3968\n",
      "Epoch 398/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.7220 - accuracy: 0.7613 - val_loss: 1.6634 - val_accuracy: 0.4233\n",
      "Epoch 399/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.7133 - accuracy: 0.7665 - val_loss: 1.6014 - val_accuracy: 0.4180\n",
      "Epoch 400/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.7195 - accuracy: 0.7678 - val_loss: 1.6352 - val_accuracy: 0.4392\n",
      "Epoch 401/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.7177 - accuracy: 0.7613 - val_loss: 1.6588 - val_accuracy: 0.4127\n",
      "Epoch 402/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.7105 - accuracy: 0.7691 - val_loss: 1.7092 - val_accuracy: 0.4127\n",
      "Epoch 403/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.7242 - accuracy: 0.7613 - val_loss: 1.6999 - val_accuracy: 0.4286\n",
      "Epoch 404/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.7072 - accuracy: 0.7730 - val_loss: 1.6659 - val_accuracy: 0.3915\n",
      "Epoch 405/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.7085 - accuracy: 0.7665 - val_loss: 1.7123 - val_accuracy: 0.4127\n",
      "Epoch 406/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.7099 - accuracy: 0.7717 - val_loss: 1.6441 - val_accuracy: 0.4127\n",
      "Epoch 407/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.7121 - accuracy: 0.7562 - val_loss: 1.6391 - val_accuracy: 0.4339\n",
      "Epoch 408/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.7080 - accuracy: 0.7730 - val_loss: 1.5874 - val_accuracy: 0.4339\n",
      "Epoch 409/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.7028 - accuracy: 0.7782 - val_loss: 1.7770 - val_accuracy: 0.3810\n",
      "Epoch 410/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.7023 - accuracy: 0.7782 - val_loss: 1.7115 - val_accuracy: 0.4286\n",
      "Epoch 411/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.6968 - accuracy: 0.7730 - val_loss: 1.7215 - val_accuracy: 0.4021\n",
      "Epoch 412/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.7060 - accuracy: 0.7704 - val_loss: 1.7041 - val_accuracy: 0.4233\n",
      "Epoch 413/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.6990 - accuracy: 0.7704 - val_loss: 1.6238 - val_accuracy: 0.3968\n",
      "Epoch 414/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.7027 - accuracy: 0.7717 - val_loss: 1.6435 - val_accuracy: 0.4550\n",
      "Epoch 415/700\n",
      "771/771 [==============================] - 3s 4ms/step - loss: 0.6915 - accuracy: 0.7601 - val_loss: 1.6423 - val_accuracy: 0.4286\n",
      "Epoch 416/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.6913 - accuracy: 0.7847 - val_loss: 1.8150 - val_accuracy: 0.3968\n",
      "Epoch 417/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.6869 - accuracy: 0.7678 - val_loss: 1.6360 - val_accuracy: 0.4392\n",
      "Epoch 418/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.6904 - accuracy: 0.7769 - val_loss: 1.6418 - val_accuracy: 0.4074\n",
      "Epoch 419/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.6778 - accuracy: 0.7834 - val_loss: 1.6775 - val_accuracy: 0.4127\n",
      "Epoch 420/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.6846 - accuracy: 0.7743 - val_loss: 1.6951 - val_accuracy: 0.4021\n",
      "Epoch 421/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.6897 - accuracy: 0.7730 - val_loss: 1.7304 - val_accuracy: 0.3915\n",
      "Epoch 422/700\n",
      "771/771 [==============================] - 3s 4ms/step - loss: 0.6932 - accuracy: 0.7704 - val_loss: 1.7442 - val_accuracy: 0.4127\n",
      "Epoch 423/700\n",
      "771/771 [==============================] - 3s 4ms/step - loss: 0.6790 - accuracy: 0.7795 - val_loss: 1.6947 - val_accuracy: 0.3968\n",
      "Epoch 424/700\n",
      "771/771 [==============================] - 3s 5ms/step - loss: 0.6803 - accuracy: 0.7782 - val_loss: 1.6747 - val_accuracy: 0.4339\n",
      "Epoch 425/700\n",
      "771/771 [==============================] - 3s 4ms/step - loss: 0.6699 - accuracy: 0.7821 - val_loss: 1.6390 - val_accuracy: 0.4286\n",
      "Epoch 426/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.6726 - accuracy: 0.7847 - val_loss: 1.6701 - val_accuracy: 0.4127\n",
      "Epoch 427/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.6757 - accuracy: 0.7860 - val_loss: 1.7346 - val_accuracy: 0.4339\n",
      "Epoch 428/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.6688 - accuracy: 0.7743 - val_loss: 1.7362 - val_accuracy: 0.4074\n",
      "Epoch 429/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.6702 - accuracy: 0.7860 - val_loss: 1.6412 - val_accuracy: 0.4233\n",
      "Epoch 430/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.6621 - accuracy: 0.7860 - val_loss: 1.7498 - val_accuracy: 0.3968\n",
      "Epoch 431/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.6634 - accuracy: 0.7743 - val_loss: 1.6803 - val_accuracy: 0.4444\n",
      "Epoch 432/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.6574 - accuracy: 0.7860 - val_loss: 1.8323 - val_accuracy: 0.3915\n",
      "Epoch 433/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.6686 - accuracy: 0.7821 - val_loss: 1.6895 - val_accuracy: 0.4074\n",
      "Epoch 434/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.6524 - accuracy: 0.7938 - val_loss: 1.7966 - val_accuracy: 0.3915\n",
      "Epoch 435/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.6604 - accuracy: 0.7886 - val_loss: 1.6628 - val_accuracy: 0.4339\n",
      "Epoch 436/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.6561 - accuracy: 0.7938 - val_loss: 1.7228 - val_accuracy: 0.4392\n",
      "Epoch 437/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.6509 - accuracy: 0.7860 - val_loss: 1.7043 - val_accuracy: 0.4180\n",
      "Epoch 438/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.6508 - accuracy: 0.7847 - val_loss: 1.6932 - val_accuracy: 0.4233\n",
      "Epoch 439/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.6527 - accuracy: 0.7964 - val_loss: 1.7664 - val_accuracy: 0.4074\n",
      "Epoch 440/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.6556 - accuracy: 0.7899 - val_loss: 1.7131 - val_accuracy: 0.4339\n",
      "Epoch 441/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.6441 - accuracy: 0.7873 - val_loss: 1.6943 - val_accuracy: 0.3968\n",
      "Epoch 442/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.6341 - accuracy: 0.8067 - val_loss: 1.8842 - val_accuracy: 0.4021\n",
      "Epoch 443/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.6508 - accuracy: 0.7938 - val_loss: 1.6829 - val_accuracy: 0.4286\n",
      "Epoch 444/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.6500 - accuracy: 0.7964 - val_loss: 1.8736 - val_accuracy: 0.3915\n",
      "Epoch 445/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.6417 - accuracy: 0.7912 - val_loss: 1.6925 - val_accuracy: 0.4180\n",
      "Epoch 446/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.6391 - accuracy: 0.7964 - val_loss: 1.7583 - val_accuracy: 0.4021\n",
      "Epoch 447/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.6258 - accuracy: 0.8003 - val_loss: 1.7436 - val_accuracy: 0.4127\n",
      "Epoch 448/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.6414 - accuracy: 0.7964 - val_loss: 1.8038 - val_accuracy: 0.3968\n",
      "Epoch 449/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.6390 - accuracy: 0.7990 - val_loss: 1.7389 - val_accuracy: 0.4180\n",
      "Epoch 450/700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "771/771 [==============================] - 4s 5ms/step - loss: 0.6352 - accuracy: 0.7912 - val_loss: 1.9038 - val_accuracy: 0.3862\n",
      "Epoch 451/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.6300 - accuracy: 0.8042 - val_loss: 1.6766 - val_accuracy: 0.4286\n",
      "Epoch 452/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.6377 - accuracy: 0.7886 - val_loss: 1.6969 - val_accuracy: 0.4021\n",
      "Epoch 453/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.6331 - accuracy: 0.8080 - val_loss: 1.6797 - val_accuracy: 0.4497\n",
      "Epoch 454/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.6274 - accuracy: 0.8042 - val_loss: 1.7598 - val_accuracy: 0.4339\n",
      "Epoch 455/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.6258 - accuracy: 0.7977 - val_loss: 1.6898 - val_accuracy: 0.4444\n",
      "Epoch 456/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.6206 - accuracy: 0.7990 - val_loss: 1.7102 - val_accuracy: 0.4180\n",
      "Epoch 457/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.6289 - accuracy: 0.8029 - val_loss: 1.6631 - val_accuracy: 0.4021\n",
      "Epoch 458/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.6208 - accuracy: 0.7938 - val_loss: 1.6738 - val_accuracy: 0.4180\n",
      "Epoch 459/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.6201 - accuracy: 0.7912 - val_loss: 1.7193 - val_accuracy: 0.4021\n",
      "Epoch 460/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.6191 - accuracy: 0.7964 - val_loss: 1.7373 - val_accuracy: 0.4339\n",
      "Epoch 461/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.6114 - accuracy: 0.7977 - val_loss: 1.7193 - val_accuracy: 0.4233\n",
      "Epoch 462/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.6091 - accuracy: 0.8197 - val_loss: 1.7256 - val_accuracy: 0.4339\n",
      "Epoch 463/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.6143 - accuracy: 0.8029 - val_loss: 1.7542 - val_accuracy: 0.4180\n",
      "Epoch 464/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.6141 - accuracy: 0.7912 - val_loss: 1.6484 - val_accuracy: 0.4180\n",
      "Epoch 465/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.6102 - accuracy: 0.8106 - val_loss: 1.6758 - val_accuracy: 0.4074\n",
      "Epoch 466/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.6076 - accuracy: 0.8132 - val_loss: 1.6877 - val_accuracy: 0.4286\n",
      "Epoch 467/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.6015 - accuracy: 0.7925 - val_loss: 1.6973 - val_accuracy: 0.4074\n",
      "Epoch 468/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.6078 - accuracy: 0.8119 - val_loss: 1.7229 - val_accuracy: 0.4286\n",
      "Epoch 469/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.5954 - accuracy: 0.8016 - val_loss: 1.7765 - val_accuracy: 0.4444\n",
      "Epoch 470/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.6011 - accuracy: 0.7977 - val_loss: 1.6825 - val_accuracy: 0.4497\n",
      "Epoch 471/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.5995 - accuracy: 0.8067 - val_loss: 1.7222 - val_accuracy: 0.4233\n",
      "Epoch 472/700\n",
      "771/771 [==============================] - 3s 4ms/step - loss: 0.5993 - accuracy: 0.8093 - val_loss: 1.6615 - val_accuracy: 0.4180\n",
      "Epoch 473/700\n",
      "771/771 [==============================] - 3s 4ms/step - loss: 0.5942 - accuracy: 0.8171 - val_loss: 1.7229 - val_accuracy: 0.4392\n",
      "Epoch 474/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.5903 - accuracy: 0.8132 - val_loss: 1.6949 - val_accuracy: 0.4127\n",
      "Epoch 475/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.5989 - accuracy: 0.8042 - val_loss: 1.7014 - val_accuracy: 0.4127\n",
      "Epoch 476/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.5964 - accuracy: 0.8067 - val_loss: 1.6805 - val_accuracy: 0.4392\n",
      "Epoch 477/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.6004 - accuracy: 0.7977 - val_loss: 1.7294 - val_accuracy: 0.4074\n",
      "Epoch 478/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.5932 - accuracy: 0.8093 - val_loss: 1.7553 - val_accuracy: 0.4074\n",
      "Epoch 479/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.5825 - accuracy: 0.8197 - val_loss: 1.6954 - val_accuracy: 0.4180\n",
      "Epoch 480/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.5890 - accuracy: 0.8210 - val_loss: 1.6893 - val_accuracy: 0.4233\n",
      "Epoch 481/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.5832 - accuracy: 0.8184 - val_loss: 1.8183 - val_accuracy: 0.4286\n",
      "Epoch 482/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.5854 - accuracy: 0.8132 - val_loss: 1.7620 - val_accuracy: 0.3968\n",
      "Epoch 483/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.5749 - accuracy: 0.8210 - val_loss: 1.7767 - val_accuracy: 0.4021\n",
      "Epoch 484/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.5784 - accuracy: 0.8171 - val_loss: 1.7565 - val_accuracy: 0.4127\n",
      "Epoch 485/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.5801 - accuracy: 0.8106 - val_loss: 1.6636 - val_accuracy: 0.4286\n",
      "Epoch 486/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.5779 - accuracy: 0.8067 - val_loss: 1.6703 - val_accuracy: 0.4286\n",
      "Epoch 487/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.5798 - accuracy: 0.8119 - val_loss: 1.6897 - val_accuracy: 0.4233\n",
      "Epoch 488/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.5705 - accuracy: 0.8249 - val_loss: 1.6852 - val_accuracy: 0.4286\n",
      "Epoch 489/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.5643 - accuracy: 0.8158 - val_loss: 1.8273 - val_accuracy: 0.4339\n",
      "Epoch 490/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.5696 - accuracy: 0.8132 - val_loss: 1.8323 - val_accuracy: 0.4180\n",
      "Epoch 491/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.5780 - accuracy: 0.8054 - val_loss: 1.6974 - val_accuracy: 0.4180\n",
      "Epoch 492/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.5659 - accuracy: 0.8184 - val_loss: 1.6903 - val_accuracy: 0.4286\n",
      "Epoch 493/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.5609 - accuracy: 0.8197 - val_loss: 1.7374 - val_accuracy: 0.4074\n",
      "Epoch 494/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.5651 - accuracy: 0.8249 - val_loss: 1.7185 - val_accuracy: 0.4233\n",
      "Epoch 495/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.5530 - accuracy: 0.8197 - val_loss: 1.8623 - val_accuracy: 0.4021\n",
      "Epoch 496/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.5499 - accuracy: 0.8236 - val_loss: 1.7481 - val_accuracy: 0.4180\n",
      "Epoch 497/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.5518 - accuracy: 0.8262 - val_loss: 1.7941 - val_accuracy: 0.4286\n",
      "Epoch 498/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.5498 - accuracy: 0.8236 - val_loss: 1.7310 - val_accuracy: 0.4021\n",
      "Epoch 499/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.5579 - accuracy: 0.8184 - val_loss: 1.7724 - val_accuracy: 0.4180\n",
      "Epoch 500/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.5534 - accuracy: 0.8314 - val_loss: 1.7229 - val_accuracy: 0.4339\n",
      "Epoch 501/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.5467 - accuracy: 0.8184 - val_loss: 1.8158 - val_accuracy: 0.4180\n",
      "Epoch 502/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.5410 - accuracy: 0.8327 - val_loss: 1.7424 - val_accuracy: 0.4392\n",
      "Epoch 503/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.5445 - accuracy: 0.8366 - val_loss: 1.7134 - val_accuracy: 0.4180\n",
      "Epoch 504/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.5393 - accuracy: 0.8210 - val_loss: 1.8501 - val_accuracy: 0.4233\n",
      "Epoch 505/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.5383 - accuracy: 0.8223 - val_loss: 1.8571 - val_accuracy: 0.3915\n",
      "Epoch 506/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.5500 - accuracy: 0.8223 - val_loss: 1.7312 - val_accuracy: 0.4127\n",
      "Epoch 507/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.5345 - accuracy: 0.8366 - val_loss: 1.7660 - val_accuracy: 0.4233\n",
      "Epoch 508/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.5431 - accuracy: 0.8223 - val_loss: 1.7777 - val_accuracy: 0.3968\n",
      "Epoch 509/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.5313 - accuracy: 0.8418 - val_loss: 1.7550 - val_accuracy: 0.4074\n",
      "Epoch 510/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.5345 - accuracy: 0.8366 - val_loss: 1.7671 - val_accuracy: 0.4180\n",
      "Epoch 511/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.5266 - accuracy: 0.8327 - val_loss: 1.8547 - val_accuracy: 0.4286\n",
      "Epoch 512/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.5322 - accuracy: 0.8327 - val_loss: 1.7244 - val_accuracy: 0.4074\n",
      "Epoch 513/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.5376 - accuracy: 0.8353 - val_loss: 1.7389 - val_accuracy: 0.4233\n",
      "Epoch 514/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.5256 - accuracy: 0.8327 - val_loss: 1.7027 - val_accuracy: 0.4180\n",
      "Epoch 515/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.5173 - accuracy: 0.8457 - val_loss: 1.7245 - val_accuracy: 0.3968\n",
      "Epoch 516/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.5263 - accuracy: 0.8210 - val_loss: 1.7833 - val_accuracy: 0.4180\n",
      "Epoch 517/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.5329 - accuracy: 0.8327 - val_loss: 1.7082 - val_accuracy: 0.4233\n",
      "Epoch 518/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.5154 - accuracy: 0.8495 - val_loss: 1.7275 - val_accuracy: 0.4233\n",
      "Epoch 519/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.5083 - accuracy: 0.8482 - val_loss: 1.8049 - val_accuracy: 0.4339\n",
      "Epoch 520/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.5219 - accuracy: 0.8418 - val_loss: 1.8629 - val_accuracy: 0.4021\n",
      "Epoch 521/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.5176 - accuracy: 0.8379 - val_loss: 1.7978 - val_accuracy: 0.4127\n",
      "Epoch 522/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.5152 - accuracy: 0.8560 - val_loss: 1.7383 - val_accuracy: 0.4180\n",
      "Epoch 523/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.5129 - accuracy: 0.8366 - val_loss: 1.7777 - val_accuracy: 0.4074\n",
      "Epoch 524/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.5120 - accuracy: 0.8457 - val_loss: 1.7587 - val_accuracy: 0.4444\n",
      "Epoch 525/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.5104 - accuracy: 0.8405 - val_loss: 1.9187 - val_accuracy: 0.3915\n",
      "Epoch 526/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.5049 - accuracy: 0.8457 - val_loss: 1.7730 - val_accuracy: 0.4127\n",
      "Epoch 527/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.5043 - accuracy: 0.8457 - val_loss: 1.8438 - val_accuracy: 0.4180\n",
      "Epoch 528/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.5059 - accuracy: 0.8379 - val_loss: 1.8188 - val_accuracy: 0.3968\n",
      "Epoch 529/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.5094 - accuracy: 0.8392 - val_loss: 1.7829 - val_accuracy: 0.4074\n",
      "Epoch 530/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.4984 - accuracy: 0.8508 - val_loss: 1.8178 - val_accuracy: 0.4180\n",
      "Epoch 531/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.5015 - accuracy: 0.8431 - val_loss: 1.8102 - val_accuracy: 0.3968\n",
      "Epoch 532/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.4979 - accuracy: 0.8482 - val_loss: 1.8400 - val_accuracy: 0.3862\n",
      "Epoch 533/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.4966 - accuracy: 0.8470 - val_loss: 1.7573 - val_accuracy: 0.4180\n",
      "Epoch 534/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.4812 - accuracy: 0.8573 - val_loss: 1.7772 - val_accuracy: 0.4127\n",
      "Epoch 535/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.4893 - accuracy: 0.8534 - val_loss: 1.7359 - val_accuracy: 0.4127\n",
      "Epoch 536/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.4975 - accuracy: 0.8405 - val_loss: 1.7908 - val_accuracy: 0.4074\n",
      "Epoch 537/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.4934 - accuracy: 0.8444 - val_loss: 1.7913 - val_accuracy: 0.4180\n",
      "Epoch 538/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.4836 - accuracy: 0.8547 - val_loss: 1.8881 - val_accuracy: 0.4233\n",
      "Epoch 539/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.4921 - accuracy: 0.8405 - val_loss: 1.7967 - val_accuracy: 0.3968\n",
      "Epoch 540/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.4847 - accuracy: 0.8547 - val_loss: 1.8093 - val_accuracy: 0.4180\n",
      "Epoch 541/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.4767 - accuracy: 0.8547 - val_loss: 1.7614 - val_accuracy: 0.4180\n",
      "Epoch 542/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.4813 - accuracy: 0.8521 - val_loss: 1.7490 - val_accuracy: 0.4233\n",
      "Epoch 543/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.4817 - accuracy: 0.8560 - val_loss: 1.7657 - val_accuracy: 0.4286\n",
      "Epoch 544/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.4756 - accuracy: 0.8482 - val_loss: 1.7711 - val_accuracy: 0.4074\n",
      "Epoch 545/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.4678 - accuracy: 0.8612 - val_loss: 1.9130 - val_accuracy: 0.3968\n",
      "Epoch 546/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.4814 - accuracy: 0.8586 - val_loss: 1.7969 - val_accuracy: 0.4074\n",
      "Epoch 547/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.4712 - accuracy: 0.8599 - val_loss: 1.8868 - val_accuracy: 0.3915\n",
      "Epoch 548/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.4665 - accuracy: 0.8573 - val_loss: 1.8421 - val_accuracy: 0.4339\n",
      "Epoch 549/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.4727 - accuracy: 0.8521 - val_loss: 1.8433 - val_accuracy: 0.3968\n",
      "Epoch 550/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.4658 - accuracy: 0.8573 - val_loss: 1.8545 - val_accuracy: 0.4339\n",
      "Epoch 551/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.4600 - accuracy: 0.8625 - val_loss: 1.7931 - val_accuracy: 0.4233\n",
      "Epoch 552/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.4595 - accuracy: 0.8599 - val_loss: 1.7957 - val_accuracy: 0.4444\n",
      "Epoch 553/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.4643 - accuracy: 0.8508 - val_loss: 1.8066 - val_accuracy: 0.4180\n",
      "Epoch 554/700\n",
      "771/771 [==============================] - 3s 4ms/step - loss: 0.4581 - accuracy: 0.8625 - val_loss: 1.8255 - val_accuracy: 0.3968\n",
      "Epoch 555/700\n",
      "771/771 [==============================] - 3s 4ms/step - loss: 0.4642 - accuracy: 0.8599 - val_loss: 1.8749 - val_accuracy: 0.4021\n",
      "Epoch 556/700\n",
      "771/771 [==============================] - 3s 4ms/step - loss: 0.4542 - accuracy: 0.8690 - val_loss: 1.8953 - val_accuracy: 0.3968\n",
      "Epoch 557/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.4546 - accuracy: 0.8638 - val_loss: 1.8308 - val_accuracy: 0.4180\n",
      "Epoch 558/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.4582 - accuracy: 0.8651 - val_loss: 1.8013 - val_accuracy: 0.4127\n",
      "Epoch 559/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.4517 - accuracy: 0.8742 - val_loss: 1.7906 - val_accuracy: 0.4233\n",
      "Epoch 560/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.4501 - accuracy: 0.8612 - val_loss: 1.7914 - val_accuracy: 0.4127\n",
      "Epoch 561/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.4602 - accuracy: 0.8586 - val_loss: 1.8112 - val_accuracy: 0.4180\n",
      "Epoch 562/700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "771/771 [==============================] - 4s 5ms/step - loss: 0.4454 - accuracy: 0.8573 - val_loss: 1.7994 - val_accuracy: 0.4233\n",
      "Epoch 563/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.4509 - accuracy: 0.8664 - val_loss: 1.8378 - val_accuracy: 0.4074\n",
      "Epoch 564/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.4418 - accuracy: 0.8781 - val_loss: 1.7877 - val_accuracy: 0.4444\n",
      "Epoch 565/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.4382 - accuracy: 0.8781 - val_loss: 1.8994 - val_accuracy: 0.4074\n",
      "Epoch 566/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.4489 - accuracy: 0.8703 - val_loss: 1.8978 - val_accuracy: 0.4286\n",
      "Epoch 567/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.4243 - accuracy: 0.8859 - val_loss: 1.9324 - val_accuracy: 0.4127\n",
      "Epoch 568/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.4350 - accuracy: 0.8742 - val_loss: 1.8440 - val_accuracy: 0.4127\n",
      "Epoch 569/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.4388 - accuracy: 0.8755 - val_loss: 1.7604 - val_accuracy: 0.3915\n",
      "Epoch 570/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.4394 - accuracy: 0.8625 - val_loss: 1.8056 - val_accuracy: 0.3862\n",
      "Epoch 571/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.4267 - accuracy: 0.8807 - val_loss: 1.8782 - val_accuracy: 0.4286\n",
      "Epoch 572/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.4296 - accuracy: 0.8768 - val_loss: 1.9077 - val_accuracy: 0.4021\n",
      "Epoch 573/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.4342 - accuracy: 0.8755 - val_loss: 1.8095 - val_accuracy: 0.4074\n",
      "Epoch 574/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.4337 - accuracy: 0.8690 - val_loss: 1.8598 - val_accuracy: 0.3915\n",
      "Epoch 575/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.4182 - accuracy: 0.8755 - val_loss: 1.8046 - val_accuracy: 0.4127\n",
      "Epoch 576/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.4220 - accuracy: 0.8820 - val_loss: 1.8400 - val_accuracy: 0.4233\n",
      "Epoch 577/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.4222 - accuracy: 0.8612 - val_loss: 1.8993 - val_accuracy: 0.4021\n",
      "Epoch 578/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.4200 - accuracy: 0.8781 - val_loss: 1.9210 - val_accuracy: 0.4074\n",
      "Epoch 579/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.4233 - accuracy: 0.8664 - val_loss: 1.8744 - val_accuracy: 0.4444\n",
      "Epoch 580/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.4188 - accuracy: 0.8677 - val_loss: 1.8692 - val_accuracy: 0.4180\n",
      "Epoch 581/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.4204 - accuracy: 0.8742 - val_loss: 1.8442 - val_accuracy: 0.4497\n",
      "Epoch 582/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.4166 - accuracy: 0.8898 - val_loss: 1.8994 - val_accuracy: 0.4021\n",
      "Epoch 583/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.4155 - accuracy: 0.8742 - val_loss: 1.9287 - val_accuracy: 0.4286\n",
      "Epoch 584/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.4158 - accuracy: 0.8781 - val_loss: 1.8191 - val_accuracy: 0.4021\n",
      "Epoch 585/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.4114 - accuracy: 0.8742 - val_loss: 1.8410 - val_accuracy: 0.4021\n",
      "Epoch 586/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.4133 - accuracy: 0.8846 - val_loss: 1.8071 - val_accuracy: 0.4233\n",
      "Epoch 587/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.4054 - accuracy: 0.8859 - val_loss: 1.9482 - val_accuracy: 0.4074\n",
      "Epoch 588/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.4069 - accuracy: 0.8794 - val_loss: 1.8039 - val_accuracy: 0.3968\n",
      "Epoch 589/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.4015 - accuracy: 0.8807 - val_loss: 1.9333 - val_accuracy: 0.4286\n",
      "Epoch 590/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.4071 - accuracy: 0.8820 - val_loss: 1.9241 - val_accuracy: 0.4286\n",
      "Epoch 591/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.4068 - accuracy: 0.8833 - val_loss: 1.8850 - val_accuracy: 0.3968\n",
      "Epoch 592/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.4015 - accuracy: 0.8885 - val_loss: 1.9500 - val_accuracy: 0.4074\n",
      "Epoch 593/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.4038 - accuracy: 0.8833 - val_loss: 1.8071 - val_accuracy: 0.4127\n",
      "Epoch 594/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.3997 - accuracy: 0.8794 - val_loss: 1.9793 - val_accuracy: 0.3862\n",
      "Epoch 595/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.3909 - accuracy: 0.8846 - val_loss: 1.9043 - val_accuracy: 0.4180\n",
      "Epoch 596/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.3942 - accuracy: 0.8911 - val_loss: 1.8340 - val_accuracy: 0.4180\n",
      "Epoch 597/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.3930 - accuracy: 0.8820 - val_loss: 1.8357 - val_accuracy: 0.4180\n",
      "Epoch 598/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.3890 - accuracy: 0.8911 - val_loss: 1.9015 - val_accuracy: 0.3968\n",
      "Epoch 599/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.3841 - accuracy: 0.8988 - val_loss: 1.9575 - val_accuracy: 0.4180\n",
      "Epoch 600/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.3862 - accuracy: 0.8962 - val_loss: 1.9181 - val_accuracy: 0.4180\n",
      "Epoch 601/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.3825 - accuracy: 0.8872 - val_loss: 1.8938 - val_accuracy: 0.4127\n",
      "Epoch 602/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.3872 - accuracy: 0.8885 - val_loss: 1.8799 - val_accuracy: 0.4127\n",
      "Epoch 603/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.3811 - accuracy: 0.8885 - val_loss: 1.8536 - val_accuracy: 0.4021\n",
      "Epoch 604/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.3712 - accuracy: 0.8988 - val_loss: 1.9000 - val_accuracy: 0.4074\n",
      "Epoch 605/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.3715 - accuracy: 0.8872 - val_loss: 1.8414 - val_accuracy: 0.4127\n",
      "Epoch 606/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.3763 - accuracy: 0.8949 - val_loss: 1.8402 - val_accuracy: 0.4286\n",
      "Epoch 607/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.3782 - accuracy: 0.8885 - val_loss: 1.8770 - val_accuracy: 0.4180\n",
      "Epoch 608/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.3693 - accuracy: 0.8988 - val_loss: 1.9286 - val_accuracy: 0.4180\n",
      "Epoch 609/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.3690 - accuracy: 0.8936 - val_loss: 1.9765 - val_accuracy: 0.3968\n",
      "Epoch 610/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.3705 - accuracy: 0.8962 - val_loss: 1.9322 - val_accuracy: 0.4127\n",
      "Epoch 611/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.3661 - accuracy: 0.8975 - val_loss: 1.9513 - val_accuracy: 0.4074\n",
      "Epoch 612/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.3711 - accuracy: 0.8833 - val_loss: 1.8778 - val_accuracy: 0.4074\n",
      "Epoch 613/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.3607 - accuracy: 0.9079 - val_loss: 1.9203 - val_accuracy: 0.4127\n",
      "Epoch 614/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.3596 - accuracy: 0.8988 - val_loss: 1.9292 - val_accuracy: 0.4444\n",
      "Epoch 615/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.3681 - accuracy: 0.9027 - val_loss: 1.8901 - val_accuracy: 0.4339\n",
      "Epoch 616/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.3579 - accuracy: 0.9014 - val_loss: 1.9151 - val_accuracy: 0.4127\n",
      "Epoch 617/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.3601 - accuracy: 0.9014 - val_loss: 2.0473 - val_accuracy: 0.4127\n",
      "Epoch 618/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.3551 - accuracy: 0.9066 - val_loss: 1.9145 - val_accuracy: 0.4074\n",
      "Epoch 619/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.3459 - accuracy: 0.9209 - val_loss: 2.0341 - val_accuracy: 0.3968\n",
      "Epoch 620/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.3572 - accuracy: 0.9040 - val_loss: 1.8757 - val_accuracy: 0.4127\n",
      "Epoch 621/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.3575 - accuracy: 0.8962 - val_loss: 1.8788 - val_accuracy: 0.3968\n",
      "Epoch 622/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.3515 - accuracy: 0.9027 - val_loss: 1.9248 - val_accuracy: 0.4074\n",
      "Epoch 623/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.3531 - accuracy: 0.9066 - val_loss: 1.9092 - val_accuracy: 0.4180\n",
      "Epoch 624/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.3493 - accuracy: 0.9053 - val_loss: 1.9669 - val_accuracy: 0.4021\n",
      "Epoch 625/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.3469 - accuracy: 0.9066 - val_loss: 1.9814 - val_accuracy: 0.4180\n",
      "Epoch 626/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.3428 - accuracy: 0.9131 - val_loss: 1.9752 - val_accuracy: 0.4286\n",
      "Epoch 627/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.3518 - accuracy: 0.9053 - val_loss: 1.9668 - val_accuracy: 0.4286\n",
      "Epoch 628/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.3339 - accuracy: 0.9144 - val_loss: 1.9289 - val_accuracy: 0.3862\n",
      "Epoch 629/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.3421 - accuracy: 0.9001 - val_loss: 1.9596 - val_accuracy: 0.4074\n",
      "Epoch 630/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.3409 - accuracy: 0.9040 - val_loss: 1.9129 - val_accuracy: 0.4392\n",
      "Epoch 631/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.3411 - accuracy: 0.9001 - val_loss: 1.9634 - val_accuracy: 0.4127\n",
      "Epoch 632/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.3223 - accuracy: 0.9261 - val_loss: 2.0085 - val_accuracy: 0.4180\n",
      "Epoch 633/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.3290 - accuracy: 0.9170 - val_loss: 1.9401 - val_accuracy: 0.4233\n",
      "Epoch 634/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.3310 - accuracy: 0.9092 - val_loss: 1.8940 - val_accuracy: 0.3968\n",
      "Epoch 635/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.3296 - accuracy: 0.9105 - val_loss: 1.9270 - val_accuracy: 0.4180\n",
      "Epoch 636/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.3286 - accuracy: 0.9131 - val_loss: 1.9426 - val_accuracy: 0.3968\n",
      "Epoch 637/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.3278 - accuracy: 0.9131 - val_loss: 1.9126 - val_accuracy: 0.4180\n",
      "Epoch 638/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.3299 - accuracy: 0.9092 - val_loss: 1.9210 - val_accuracy: 0.4339\n",
      "Epoch 639/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.3189 - accuracy: 0.9326 - val_loss: 2.0654 - val_accuracy: 0.4021\n",
      "Epoch 640/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.3211 - accuracy: 0.9053 - val_loss: 2.0400 - val_accuracy: 0.4127\n",
      "Epoch 641/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.3253 - accuracy: 0.9118 - val_loss: 1.9317 - val_accuracy: 0.3968\n",
      "Epoch 642/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.3161 - accuracy: 0.9248 - val_loss: 1.9419 - val_accuracy: 0.4127\n",
      "Epoch 643/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.3250 - accuracy: 0.9157 - val_loss: 2.0646 - val_accuracy: 0.4074\n",
      "Epoch 644/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.3207 - accuracy: 0.9274 - val_loss: 2.0227 - val_accuracy: 0.4127\n",
      "Epoch 645/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.3160 - accuracy: 0.9222 - val_loss: 2.0146 - val_accuracy: 0.4286\n",
      "Epoch 646/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.3124 - accuracy: 0.9183 - val_loss: 2.0782 - val_accuracy: 0.4074\n",
      "Epoch 647/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.3144 - accuracy: 0.9235 - val_loss: 2.0398 - val_accuracy: 0.4074\n",
      "Epoch 648/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.3104 - accuracy: 0.9248 - val_loss: 1.9693 - val_accuracy: 0.4074\n",
      "Epoch 649/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.3038 - accuracy: 0.9274 - val_loss: 1.9449 - val_accuracy: 0.3915\n",
      "Epoch 650/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.3139 - accuracy: 0.9144 - val_loss: 2.1696 - val_accuracy: 0.4074\n",
      "Epoch 651/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.3125 - accuracy: 0.9118 - val_loss: 1.9793 - val_accuracy: 0.3968\n",
      "Epoch 652/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.3040 - accuracy: 0.9235 - val_loss: 1.9863 - val_accuracy: 0.4233\n",
      "Epoch 653/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.3004 - accuracy: 0.9183 - val_loss: 2.0169 - val_accuracy: 0.4074\n",
      "Epoch 654/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.3031 - accuracy: 0.9222 - val_loss: 2.0651 - val_accuracy: 0.4021\n",
      "Epoch 655/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.2994 - accuracy: 0.9339 - val_loss: 1.9855 - val_accuracy: 0.4127\n",
      "Epoch 656/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.2991 - accuracy: 0.9274 - val_loss: 1.9839 - val_accuracy: 0.4286\n",
      "Epoch 657/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.3011 - accuracy: 0.9261 - val_loss: 2.0047 - val_accuracy: 0.4286\n",
      "Epoch 658/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.2999 - accuracy: 0.9274 - val_loss: 1.9941 - val_accuracy: 0.4180\n",
      "Epoch 659/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.2941 - accuracy: 0.9261 - val_loss: 2.1516 - val_accuracy: 0.4021\n",
      "Epoch 660/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.2954 - accuracy: 0.9313 - val_loss: 2.0683 - val_accuracy: 0.4074\n",
      "Epoch 661/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.2932 - accuracy: 0.9339 - val_loss: 2.0677 - val_accuracy: 0.4127\n",
      "Epoch 662/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.2853 - accuracy: 0.9287 - val_loss: 2.0553 - val_accuracy: 0.4180\n",
      "Epoch 663/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.2783 - accuracy: 0.9274 - val_loss: 2.0836 - val_accuracy: 0.4339\n",
      "Epoch 664/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.2854 - accuracy: 0.9274 - val_loss: 2.1512 - val_accuracy: 0.4021\n",
      "Epoch 665/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.2838 - accuracy: 0.9261 - val_loss: 2.0285 - val_accuracy: 0.4286\n",
      "Epoch 666/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.2808 - accuracy: 0.9326 - val_loss: 2.0380 - val_accuracy: 0.4074\n",
      "Epoch 667/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.2825 - accuracy: 0.9364 - val_loss: 2.0528 - val_accuracy: 0.4233\n",
      "Epoch 668/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.2790 - accuracy: 0.9326 - val_loss: 2.0113 - val_accuracy: 0.3968\n",
      "Epoch 669/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.2848 - accuracy: 0.9313 - val_loss: 2.0347 - val_accuracy: 0.4074\n",
      "Epoch 670/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.2832 - accuracy: 0.9196 - val_loss: 1.9990 - val_accuracy: 0.4074\n",
      "Epoch 671/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.2800 - accuracy: 0.9377 - val_loss: 2.0056 - val_accuracy: 0.4286\n",
      "Epoch 672/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.2809 - accuracy: 0.9222 - val_loss: 2.0797 - val_accuracy: 0.3915\n",
      "Epoch 673/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.2790 - accuracy: 0.9313 - val_loss: 2.0409 - val_accuracy: 0.4127\n",
      "Epoch 674/700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "771/771 [==============================] - 4s 5ms/step - loss: 0.2800 - accuracy: 0.9326 - val_loss: 2.0223 - val_accuracy: 0.4021\n",
      "Epoch 675/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.2643 - accuracy: 0.9287 - val_loss: 2.0703 - val_accuracy: 0.4180\n",
      "Epoch 676/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.2699 - accuracy: 0.9364 - val_loss: 2.0985 - val_accuracy: 0.4021\n",
      "Epoch 677/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.2724 - accuracy: 0.9403 - val_loss: 2.0471 - val_accuracy: 0.4074\n",
      "Epoch 678/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.2647 - accuracy: 0.9442 - val_loss: 2.0597 - val_accuracy: 0.4180\n",
      "Epoch 679/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.2597 - accuracy: 0.9494 - val_loss: 2.0692 - val_accuracy: 0.3810\n",
      "Epoch 680/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.2578 - accuracy: 0.9390 - val_loss: 2.0624 - val_accuracy: 0.4127\n",
      "Epoch 681/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.2647 - accuracy: 0.9390 - val_loss: 2.1605 - val_accuracy: 0.4021\n",
      "Epoch 682/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.2583 - accuracy: 0.9403 - val_loss: 2.0216 - val_accuracy: 0.4021\n",
      "Epoch 683/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.2658 - accuracy: 0.9339 - val_loss: 1.9953 - val_accuracy: 0.4021\n",
      "Epoch 684/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.2582 - accuracy: 0.9455 - val_loss: 2.0345 - val_accuracy: 0.4127\n",
      "Epoch 685/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.2604 - accuracy: 0.9390 - val_loss: 2.0141 - val_accuracy: 0.4021\n",
      "Epoch 686/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.2512 - accuracy: 0.9455 - val_loss: 2.1290 - val_accuracy: 0.4127\n",
      "Epoch 687/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.2503 - accuracy: 0.9429 - val_loss: 2.1310 - val_accuracy: 0.4074\n",
      "Epoch 688/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.2522 - accuracy: 0.9377 - val_loss: 2.0848 - val_accuracy: 0.4180\n",
      "Epoch 689/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.2580 - accuracy: 0.9442 - val_loss: 2.0811 - val_accuracy: 0.4074\n",
      "Epoch 690/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.2519 - accuracy: 0.9429 - val_loss: 2.1313 - val_accuracy: 0.4233\n",
      "Epoch 691/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.2477 - accuracy: 0.9429 - val_loss: 2.0905 - val_accuracy: 0.4233\n",
      "Epoch 692/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.2552 - accuracy: 0.9507 - val_loss: 2.0306 - val_accuracy: 0.4127\n",
      "Epoch 693/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.2498 - accuracy: 0.9403 - val_loss: 2.1828 - val_accuracy: 0.4021\n",
      "Epoch 694/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.2512 - accuracy: 0.9455 - val_loss: 2.1237 - val_accuracy: 0.4074\n",
      "Epoch 695/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.2491 - accuracy: 0.9455 - val_loss: 2.0936 - val_accuracy: 0.4074\n",
      "Epoch 696/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.2419 - accuracy: 0.9429 - val_loss: 2.0607 - val_accuracy: 0.3968\n",
      "Epoch 697/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.2406 - accuracy: 0.9468 - val_loss: 2.1582 - val_accuracy: 0.4127\n",
      "Epoch 698/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.2393 - accuracy: 0.9546 - val_loss: 2.0625 - val_accuracy: 0.4392\n",
      "Epoch 699/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.2350 - accuracy: 0.9481 - val_loss: 2.1027 - val_accuracy: 0.4021\n",
      "Epoch 700/700\n",
      "771/771 [==============================] - 4s 5ms/step - loss: 0.2371 - accuracy: 0.9403 - val_loss: 2.1770 - val_accuracy: 0.4021\n"
     ]
    }
   ],
   "source": [
    "cnnhistory=model.fit(x_traincnn, y_train, batch_size=16, epochs=700, validation_data=(x_testcnn, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(cnnhistory.history['loss'])\n",
    "plt.plot(cnnhistory.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved trained model at C:\\Users\\reddyvs\\Documents\\Junior-Senior Year\\New folder\\EmotionalyAwareDialogueSystem\\saved_models\\Emotion_Voice_Detection_Model.h5 \n"
     ]
    }
   ],
   "source": [
    "model_name = 'Emotion_Voice_Detection_Model.h5'\n",
    "save_dir = os.path.join(os.getcwd(), 'saved_models')\n",
    "# Save model and weights\n",
    "if not os.path.isdir(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "model_path = os.path.join(save_dir, model_name)\n",
    "model.save(model_path)\n",
    "print('Saved trained model at %s ' % model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "model_json = model.to_json()\n",
    "with open(\"model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n",
      "accuracy: 40.21%\n"
     ]
    }
   ],
   "source": [
    "# loading json and creating model\n",
    "from keras.models import model_from_json\n",
    "json_file = open('model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(\"saved_models/Emotion_Voice_Detection_Model.h5\")\n",
    "print(\"Loaded model from disk\")\n",
    " \n",
    "# evaluate loaded model on test data\n",
    "loaded_model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "score = loaded_model.evaluate(x_testcnn, y_test, verbose=0)\n",
    "print(\"%s: %.2f%%\" % (loaded_model.metrics_names[1], score[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting emotions on the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "189/189 [==============================] - 0s 1ms/step\n"
     ]
    }
   ],
   "source": [
    "preds = loaded_model.predict(x_testcnn, \n",
    "                         batch_size=32, \n",
    "                         verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.06563630e-06, 6.28070711e-06, 2.48405213e-10, ...,\n",
       "        5.45231785e-07, 2.22251813e-07, 9.30338516e-04],\n",
       "       [9.38392639e-01, 6.77755793e-07, 1.37657495e-02, ...,\n",
       "        1.31961908e-02, 2.75255530e-04, 1.02606123e-06],\n",
       "       [5.75702824e-03, 1.93552837e-01, 7.62005150e-02, ...,\n",
       "        5.66844130e-04, 7.64942961e-04, 2.20513350e-04],\n",
       "       ...,\n",
       "       [8.22816119e-02, 6.48302019e-01, 1.35348529e-01, ...,\n",
       "        1.63123715e-07, 1.37574689e-08, 2.30645105e-06],\n",
       "       [7.90032744e-03, 5.14553547e-01, 3.02732699e-02, ...,\n",
       "        5.98519901e-03, 3.45109473e-03, 7.71775795e-03],\n",
       "       [2.37382064e-03, 1.24921925e-01, 2.95009091e-02, ...,\n",
       "        4.38284304e-04, 4.62189473e-06, 2.43828428e-04]], dtype=float32)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds1=preds.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6, 0, 4, 1, 2, 8, 9, 6, 3, 1, 9, 6, 3, 9, 0, 0, 3, 6, 3, 1, 1, 1,\n",
       "       0, 1, 9, 1, 1, 1, 5, 5, 5, 5, 9, 6, 1, 5, 3, 3, 0, 5, 5, 5, 1, 3,\n",
       "       6, 1, 1, 7, 4, 1, 9, 1, 4, 7, 0, 5, 6, 3, 2, 6, 5, 7, 1, 7, 9, 5,\n",
       "       2, 7, 1, 9, 0, 7, 1, 0, 1, 0, 0, 9, 7, 1, 2, 1, 3, 0, 1, 8, 6, 3,\n",
       "       5, 0, 3, 0, 4, 6, 1, 3, 7, 3, 5, 1, 5, 3, 3, 3, 5, 3, 1, 6, 7, 0,\n",
       "       8, 8, 0, 7, 5, 6, 7, 6, 7, 6, 9, 0, 6, 8, 2, 5, 3, 6, 5, 2, 8, 4,\n",
       "       9, 0, 9, 6, 1, 0, 0, 0, 3, 2, 5, 5, 1, 1, 9, 5, 8, 1, 1, 9, 4, 6,\n",
       "       8, 5, 9, 9, 2, 0, 9, 1, 0, 3, 7, 4, 6, 0, 7, 9, 1, 4, 1, 2, 5, 6,\n",
       "       3, 5, 0, 8, 1, 5, 1, 8, 0, 0, 1, 1, 4], dtype=int64)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "abc = preds1.astype(int).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = (lb.inverse_transform((abc)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>predictedvalues</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>male_calm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>female_angry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>female_sad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>female_calm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>female_fearful</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>male_happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>male_sad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>male_calm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>female_happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>female_calm</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  predictedvalues\n",
       "0       male_calm\n",
       "1    female_angry\n",
       "2      female_sad\n",
       "3     female_calm\n",
       "4  female_fearful\n",
       "5      male_happy\n",
       "6        male_sad\n",
       "7       male_calm\n",
       "8    female_happy\n",
       "9     female_calm"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preddf = pd.DataFrame({'predictedvalues': predictions})\n",
    "preddf[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual=y_test.argmax(axis=1)\n",
    "abc123 = actual.astype(int).flatten()\n",
    "actualvalues = (lb.inverse_transform((abc123)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>actualvalues</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>male_calm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>female_sad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>female_calm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>female_calm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>female_fearful</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>male_angry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>male_sad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>male_sad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>female_angry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>female_happy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     actualvalues\n",
       "0       male_calm\n",
       "1      female_sad\n",
       "2     female_calm\n",
       "3     female_calm\n",
       "4  female_fearful\n",
       "5      male_angry\n",
       "6        male_sad\n",
       "7        male_sad\n",
       "8    female_angry\n",
       "9    female_happy"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actualdf = pd.DataFrame({'actualvalues': actualvalues})\n",
    "actualdf[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "finaldf = actualdf.join(preddf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actual v/s Predicted emotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>actualvalues</th>\n",
       "      <th>predictedvalues</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>female_fearful</td>\n",
       "      <td>female_calm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>female_sad</td>\n",
       "      <td>female_sad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>female_sad</td>\n",
       "      <td>female_calm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>female_happy</td>\n",
       "      <td>female_fearful</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>female_angry</td>\n",
       "      <td>male_angry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>male_calm</td>\n",
       "      <td>male_calm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>female_happy</td>\n",
       "      <td>female_happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>male_angry</td>\n",
       "      <td>male_angry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>female_angry</td>\n",
       "      <td>female_angry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>male_angry</td>\n",
       "      <td>male_happy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       actualvalues predictedvalues\n",
       "170  female_fearful     female_calm\n",
       "171      female_sad      female_sad\n",
       "172      female_sad     female_calm\n",
       "173    female_happy  female_fearful\n",
       "174    female_angry      male_angry\n",
       "175       male_calm       male_calm\n",
       "176    female_happy    female_happy\n",
       "177      male_angry      male_angry\n",
       "178    female_angry    female_angry\n",
       "179      male_angry      male_happy"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finaldf[170:180]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>predictedvalues</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>actualvalues</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>female_angry</th>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>female_calm</th>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>female_fearful</th>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>female_happy</th>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>female_sad</th>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>male_angry</th>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>male_calm</th>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>male_fearful</th>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>male_happy</th>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>male_sad</th>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                predictedvalues\n",
       "actualvalues                   \n",
       "female_angry                 20\n",
       "female_calm                  20\n",
       "female_fearful               17\n",
       "female_happy                 21\n",
       "female_sad                   20\n",
       "male_angry                   17\n",
       "male_calm                    12\n",
       "male_fearful                 20\n",
       "male_happy                   22\n",
       "male_sad                     20"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finaldf.groupby('actualvalues').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>actualvalues</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>predictedvalues</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>female_angry</th>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>female_calm</th>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>female_fearful</th>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>female_happy</th>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>female_sad</th>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>male_angry</th>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>male_calm</th>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>male_fearful</th>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>male_happy</th>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>male_sad</th>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 actualvalues\n",
       "predictedvalues              \n",
       "female_angry               26\n",
       "female_calm                37\n",
       "female_fearful              9\n",
       "female_happy               21\n",
       "female_sad                  9\n",
       "male_angry                 25\n",
       "male_calm                  20\n",
       "male_fearful               14\n",
       "male_happy                 10\n",
       "male_sad                   18"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finaldf.groupby('predictedvalues').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "finaldf.to_csv('Predictions.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Live Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Still need to finish creating a live demo recorder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, sampling_rate = librosa.load('output10.wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\reddyvs\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\magics\\pylab.py:160: UserWarning: pylab import has clobbered these variables: ['test', 'shuffle']\n",
      "`%matplotlib` prevents importing * from pylab and numpy\n",
      "  \"\\n`%matplotlib` prevents importing * from pylab and numpy\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PolyCollection at 0x1819a607b70>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%pylab inline\n",
    "import os\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import glob \n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "librosa.display.waveplot(data, sr=sampling_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#livedf= pd.DataFrame(columns=['feature'])\n",
    "X, sample_rate = librosa.load('output10.wav', res_type='kaiser_fast',duration=2.5,sr=22050*2,offset=0.5)\n",
    "sample_rate = np.array(sample_rate)\n",
    "mfccs = np.mean(librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=13),axis=0)\n",
    "featurelive = mfccs\n",
    "livedf2 = featurelive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "livedf2= pd.DataFrame(data=livedf2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "livedf2 = livedf2.stack().to_frame().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>206</th>\n",
       "      <th>207</th>\n",
       "      <th>208</th>\n",
       "      <th>209</th>\n",
       "      <th>210</th>\n",
       "      <th>211</th>\n",
       "      <th>212</th>\n",
       "      <th>213</th>\n",
       "      <th>214</th>\n",
       "      <th>215</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>...</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-46.122616</td>\n",
       "      <td>-48.112896</td>\n",
       "      <td>-48.131584</td>\n",
       "      <td>-48.222752</td>\n",
       "      <td>-48.132027</td>\n",
       "      <td>-47.939976</td>\n",
       "      <td>-47.812954</td>\n",
       "      <td>-47.812202</td>\n",
       "      <td>-47.812202</td>\n",
       "      <td>-47.812202</td>\n",
       "      <td>...</td>\n",
       "      <td>-30.652212</td>\n",
       "      <td>-33.252586</td>\n",
       "      <td>-33.533241</td>\n",
       "      <td>-34.851444</td>\n",
       "      <td>-38.675488</td>\n",
       "      <td>-43.682579</td>\n",
       "      <td>-47.587387</td>\n",
       "      <td>-46.945976</td>\n",
       "      <td>-46.02813</td>\n",
       "      <td>-40.943333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 216 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0          1          2          3          4          5    \\\n",
       "           0          0          0          0          0          0   \n",
       "0 -46.122616 -48.112896 -48.131584 -48.222752 -48.132027 -47.939976   \n",
       "\n",
       "         6          7          8          9    ...        206        207  \\\n",
       "           0          0          0          0  ...          0          0   \n",
       "0 -47.812954 -47.812202 -47.812202 -47.812202  ... -30.652212 -33.252586   \n",
       "\n",
       "         208        209        210        211        212        213       214  \\\n",
       "           0          0          0          0          0          0         0   \n",
       "0 -33.533241 -34.851444 -38.675488 -43.682579 -47.587387 -46.945976 -46.02813   \n",
       "\n",
       "         215  \n",
       "           0  \n",
       "0 -40.943333  \n",
       "\n",
       "[1 rows x 216 columns]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "livedf2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "twodim= np.expand_dims(livedf2, axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "1/1 [==============================] - 0s 5ms/step\n"
     ]
    }
   ],
   "source": [
    "livepreds = loaded_model.predict(twodim, \n",
    "                         batch_size=32, \n",
    "                         verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[8.8030822e-05, 7.1494716e-17, 1.6097458e-08, 5.7713322e-12,\n",
       "        3.5846604e-14, 9.9990308e-01, 1.7294879e-11, 6.9278790e-06,\n",
       "        1.8540521e-06, 2.1048509e-10]], dtype=float32)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "livepreds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "livepreds1=livepreds.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "liveabc = livepreds1.astype(int).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['male_angry'], dtype=object)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "livepredictions = (lb.inverse_transform((liveabc)))\n",
    "livepredictions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
